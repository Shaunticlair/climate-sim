{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Goal\n",
    "\n",
    "Our model is designed to compute:\n",
    "\n",
    "$x_{t+1}$ from $x_t$\n",
    "\n",
    "So we can directly predict the next state of our system given the current state.\n",
    "\n",
    "Unlike the base model, we will use a single set of fixed parameters for the entire dataset, rather than generating new parameters for each datapoint.\n",
    "\n",
    "# Our Data\n",
    "\n",
    "Our model receives:\n",
    "- $x_t$: Current state\n",
    "\n",
    "And attempts to predict:\n",
    "- $x_{t+1}$: The next state\n",
    "\n",
    "We generate our data by:\n",
    "- First establishing fixed physical parameters for the entire dataset\n",
    "- Then for each datapoint:\n",
    "  - Generating a new, random initial state $x_t$\n",
    "  - Running it forward one timestep to get $x_{t+1}$\n",
    "  - Using this pair to create one datapoint\n",
    "  - Repeat with new random initial states but same fixed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelDataset(Dataset):\n",
    "    def __init__(self, num_runs: int, timesteps_per_run: int, nr: int, nc: int, dt: float, F: float, fixed_params: dict = None):\n",
    "        \"\"\"\n",
    "        Initialize dataset with fixed parameters for all samples.\n",
    "        \n",
    "        Args:\n",
    "            num_runs (int): Number of different world configurations to generate\n",
    "            timesteps_per_run (int): Number of timesteps to simulate for each world\n",
    "            nr (int): Number of rows in grid\n",
    "            nc (int): Number of columns in grid\n",
    "            dt (float): Time step size\n",
    "            F (float): Forcing parameter\n",
    "            fixed_params (dict): Optional pre-generated parameters to use instead of generating new ones\n",
    "        \"\"\"\n",
    "        self.num_runs = num_runs\n",
    "        self.timesteps_per_run = timesteps_per_run\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        self.dt = dt\n",
    "        self.F = F\n",
    "        \n",
    "        # Either use provided parameters or generate new ones\n",
    "        self.fixed_params = fixed_params if fixed_params is not None else self._generate_params()\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def _generate_gaussian_field(self, n, nrv, ncv):\n",
    "        \"\"\"\n",
    "        Generate a Gaussian field composed of n Gaussian functions.\n",
    "        Uses same parameters as helper.py for consistency.\n",
    "        \"\"\"\n",
    "        mux = np.random.choice(ncv, n)\n",
    "        muy = np.random.choice(range(2, nrv - 2), n)\n",
    "        sigmax = np.random.uniform(1, ncv/4, n)\n",
    "        sigmay = np.random.uniform(1, nrv/4, n)\n",
    "\n",
    "        v = np.zeros((nrv, ncv))\n",
    "        for i in range(n):\n",
    "            for x in range(ncv):\n",
    "                for y in range(nrv):\n",
    "                    # Create three copies for pseudo-periodic field\n",
    "                    gauss = np.exp(-((x-mux[i])**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]-ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]+ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    v[y,x] += gauss\n",
    "        return v\n",
    "\n",
    "    def _generate_circular_field(self, v):\n",
    "        \"\"\"Generate a circular field from gradient of input field.\"\"\"\n",
    "        grad_v_y, grad_v_x = np.gradient(v)\n",
    "        return -grad_v_y, grad_v_x\n",
    "\n",
    "    def _generate_params(self):\n",
    "        \"\"\"\n",
    "        Generate fixed model parameters to be used for all samples.\n",
    "        Modified to match helper.py parameter generation.\n",
    "        \"\"\"\n",
    "        # Base grid parameters - unit spacing\n",
    "        DX_C = torch.ones(self.nr, self.nc + 1)\n",
    "        DY_C = torch.ones(self.nr + 1, self.nc)\n",
    "        DX_G = torch.ones(self.nr + 1, self.nc)\n",
    "        DY_G = torch.ones(self.nr, self.nc + 1)\n",
    "        RAC = torch.ones(self.nr, self.nc)\n",
    "\n",
    "        # Generate random diffusivities (must be positive)\n",
    "        # Using random uniform distribution between 0 and 1\n",
    "        KX = torch.rand(self.nr, self.nc + 1)\n",
    "        KY = torch.rand(self.nr + 1, self.nc)\n",
    "\n",
    "        # Generate velocities using Gaussian field approach\n",
    "        num_gauss = 16  # Number of Gaussian functions for velocity field\n",
    "        gauss = self._generate_gaussian_field(num_gauss, self.nr + 1, self.nc + 1)\n",
    "        VX_np, VY_np = self._generate_circular_field(gauss)\n",
    "        \n",
    "        # Convert velocities to PyTorch and scale\n",
    "        # Using scaling factor of 100 as in helper.py\n",
    "        VX = torch.from_numpy(1 * VX_np[:-1, :]).float()\n",
    "        VY = torch.from_numpy(1 * VY_np[:, :-1]).float()\n",
    "\n",
    "        # Generate random forcing field with controlled magnitude\n",
    "        # Using standard normal distribution scaled down\n",
    "        f = torch.randn(self.nr * self.nc) / np.sqrt(self.nr * self.nc)\n",
    "\n",
    "        return {\n",
    "            'KX': KX,\n",
    "            'KY': KY,\n",
    "            'DX_C': DX_C,\n",
    "            'DY_C': DY_C,\n",
    "            'DX_G': DX_G,\n",
    "            'DY_G': DY_G,\n",
    "            'VX': VX,\n",
    "            'VY': VY,\n",
    "            'RAC': RAC,\n",
    "            'f': f,\n",
    "        }\n",
    "\n",
    "    def _generate_samples(self):\n",
    "        \"\"\"Generate multiple samples using fixed parameters but with controlled initial states.\"\"\"\n",
    "        samples = []\n",
    "        total_samples = self.num_runs * (self.timesteps_per_run - 1)  # -1 because we need pairs\n",
    "        \n",
    "        for run in range(self.num_runs):\n",
    "            if run % 100 == 0:\n",
    "                print(f\"Generating world {run+1}/{self.num_runs}\")\n",
    "            \n",
    "            # Generate initial state with controlled magnitude\n",
    "            # Scale by 1/sqrt(n) to maintain reasonable magnitudes\n",
    "            x_current = torch.randn(self.nr * self.nc) / np.sqrt(self.nr * self.nc)\n",
    "            \n",
    "            # Generate timesteps for this world\n",
    "            world_states = [x_current]\n",
    "            for t in range(self.timesteps_per_run - 1):\n",
    "                x_next = self._compute_next_state(x_current)\n",
    "                world_states.append(x_next)\n",
    "                x_current = x_next\n",
    "            \n",
    "            # Create pairs of consecutive states as samples\n",
    "            for t in range(len(world_states) - 1):\n",
    "                samples.append((world_states[t], world_states[t + 1]))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def _compute_next_state(self, x_t):\n",
    "        \"\"\"Compute x(t+1) using fixed parameters.\"\"\"\n",
    "        # Convert PyTorch tensors to numpy arrays for helper function\n",
    "        np_params = {\n",
    "            key: tensor.cpu().detach().numpy() if torch.is_tensor(tensor) else tensor\n",
    "            for key, tensor in self.fixed_params.items()\n",
    "        }\n",
    "        \n",
    "        # Get model matrix M using helper function\n",
    "        M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "            nr=self.nr,\n",
    "            nc=self.nc,\n",
    "            dt=self.dt,\n",
    "            KX=np_params['KX'],\n",
    "            KY=np_params['KY'],\n",
    "            DX_C=np_params['DX_C'],\n",
    "            DY_C=np_params['DY_C'], \n",
    "            DX_G=np_params['DX_G'],\n",
    "            DY_G=np_params['DY_G'],\n",
    "            VX=np_params['VX'],\n",
    "            VY=np_params['VY'],\n",
    "            RAC=np_params['RAC'],\n",
    "            F=self.F,\n",
    "            cyclic_east_west=True,\n",
    "            cyclic_north_south=False,\n",
    "            M_is_sparse=False\n",
    "        )\n",
    "        \n",
    "        x_t_np = x_t.cpu().detach().numpy()\n",
    "        f_np = np_params['f']\n",
    "        \n",
    "        result_np = M @ x_t_np + self.F * f_np\n",
    "        return torch.from_numpy(result_np).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelNet(torch.nn.Module):\n",
    "    def __init__(self, nr: int, nc: int):\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size  # Now only takes state as input\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        # Original simple architecture with two layers\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 25)\n",
    "        self.fc2 = torch.nn.Linear(25, self.output_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Ensure x has correct shape\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_dataset(\n",
    "    filename: str,\n",
    "    num_train_runs: int,\n",
    "    num_test_runs: int,\n",
    "    timesteps_per_run: int,\n",
    "    nr: int,\n",
    "    nc: int,\n",
    "    dt: float,\n",
    "    F: float\n",
    ") -> None:\n",
    "    # Generate a single set of parameters first\n",
    "    temp_dataset = StateModelDataset(1, 1, nr, nc, dt, F)\n",
    "    shared_params = temp_dataset.fixed_params\n",
    "    \n",
    "    # Generate the model matrix M using the helper function\n",
    "    M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "        nr=nr,\n",
    "        nc=nc,\n",
    "        dt=dt,\n",
    "        KX=shared_params['KX'].numpy(),\n",
    "        KY=shared_params['KY'].numpy(),\n",
    "        DX_C=shared_params['DX_C'].numpy(),\n",
    "        DY_C=shared_params['DY_C'].numpy(),\n",
    "        DX_G=shared_params['DX_G'].numpy(),\n",
    "        DY_G=shared_params['DY_G'].numpy(),\n",
    "        VX=shared_params['VX'].numpy(),\n",
    "        VY=shared_params['VY'].numpy(),\n",
    "        RAC=shared_params['RAC'].numpy(),\n",
    "        F=F,\n",
    "        cyclic_east_west=True,\n",
    "        cyclic_north_south=False,\n",
    "        M_is_sparse=False\n",
    "    )\n",
    "    \n",
    "    # Create dataset instances with shared parameters\n",
    "    train_dataset = StateModelDataset(num_train_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    test_dataset = StateModelDataset(num_test_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Save top-level data that's shared between train and test\n",
    "        f.create_dataset('model_matrix', data=M)  # Store M at top level\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['nr'] = nr\n",
    "        f.attrs['nc'] = nc\n",
    "        f.attrs['dt'] = dt\n",
    "        f.attrs['F'] = F\n",
    "        f.attrs['timesteps_per_run'] = timesteps_per_run\n",
    "        \n",
    "        # Save the shared parameters\n",
    "        params_group = f.create_group('shared_params')\n",
    "        for key, value in shared_params.items():\n",
    "            params_group.create_dataset(key, data=value.numpy())\n",
    "        \n",
    "        # Create train and test groups for the actual datasets\n",
    "        train_group = f.create_group('train')\n",
    "        test_group = f.create_group('test')\n",
    "        \n",
    "        # Helper function to save a single dataset\n",
    "        def save_dataset(group, dataset, desc):\n",
    "            samples_group = group.create_group('samples')\n",
    "            for i, (x_t, x_t_plus_1) in enumerate(tqdm(dataset, desc=desc)):\n",
    "                sample_group = samples_group.create_group(f'sample_{i}')\n",
    "                sample_group.create_dataset('x_t', data=x_t.numpy())\n",
    "                sample_group.create_dataset('x_t_plus_1', data=x_t_plus_1.numpy())\n",
    "        \n",
    "        # Save training and testing datasets\n",
    "        save_dataset(train_group, train_dataset, \"Saving training data\")\n",
    "        save_dataset(test_group, test_dataset, \"Saving testing data\")\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple[Dict, Dict]:\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Load top-level shared data\n",
    "        metadata = {\n",
    "            'nr': f.attrs['nr'],\n",
    "            'nc': f.attrs['nc'],\n",
    "            'dt': f.attrs['dt'],\n",
    "            'F': f.attrs['F'],\n",
    "            'timesteps_per_run': f.attrs['timesteps_per_run']\n",
    "        }\n",
    "        \n",
    "        # Load the model matrix M from top level\n",
    "        M = f['model_matrix'][:]\n",
    "        \n",
    "        # Load shared parameters\n",
    "        shared_params = {}\n",
    "        params_group = f['shared_params']\n",
    "        for key in params_group.keys():\n",
    "            shared_params[key] = torch.from_numpy(params_group[key][:]).float()\n",
    "        \n",
    "        def load_dataset(group):\n",
    "            data = []\n",
    "            samples_group = group['samples']\n",
    "            for sample_name in tqdm(samples_group.keys(), desc=f\"Loading {group.name} data\"):\n",
    "                sample = samples_group[sample_name]\n",
    "                x_t = torch.from_numpy(sample['x_t'][:]).float()\n",
    "                x_t_plus_1 = torch.from_numpy(sample['x_t_plus_1'][:]).float()\n",
    "                data.append((x_t, x_t_plus_1))\n",
    "            return data\n",
    "        \n",
    "        train_data = load_dataset(f['train'])\n",
    "        test_data = load_dataset(f['test'])\n",
    "    \n",
    "    # Create common shared data dictionary\n",
    "    shared_data = {\n",
    "        'metadata': metadata,\n",
    "        'shared_params': shared_params,\n",
    "        'model_matrix': M\n",
    "    }\n",
    "    \n",
    "    # Return dictionaries that combine shared data with specific train/test data\n",
    "    return {\n",
    "        'train': train_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }, {\n",
    "        'test': test_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }\n",
    "\n",
    "\n",
    "class SavedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pre-saved data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def create_dataloaders(train_data, test_data, batch_size=32, shuffle_train=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader objects for training and testing datasets.\n",
    "    Now handles data without parameters.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training dataset dictionary containing 'train' data\n",
    "        test_data: Testing dataset dictionary containing 'test' data\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        shuffle_train: Whether to shuffle training data\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoader objects\n",
    "    \"\"\"\n",
    "    train_dataset = SavedDataset(train_data['train'])\n",
    "    test_dataset = SavedDataset(test_data['test'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save datasets\n",
    "\"\"\"\n",
    "num_train_runs = 2000\n",
    "num_test_runs = 200\n",
    "generate_and_save_dataset(\n",
    "    filename=f'state_transition_data_{num_train_runs}_{num_test_runs}.h5',\n",
    "    num_train_runs=num_train_runs,      # Number of different runs for training\n",
    "    num_test_runs=num_test_runs,        # Number of different runs for testing\n",
    "    timesteps_per_run=10,      # Number of timesteps per run\n",
    "    nr=10,\n",
    "    nc=10,\n",
    "    dt=0.01,\n",
    "    F=0.01\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Load dataset and create dataloaders\n",
    "train_data, test_data = load_dataset('state_model_data_one-world.h5')\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    train_data, \n",
    "    test_data, \n",
    "    batch_size=32\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: str = 'cpu',\n",
    "    run_number: int = None\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Get total dataset sizes\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    batch_size_train = train_dataloader.batch_size\n",
    "    batch_size_test = test_dataloader.batch_size\n",
    "    \n",
    "    run_str = f\" (Run {run_number})\" if run_number is not None else \"\"\n",
    "    print(f\"Training started{run_str}...\")\n",
    "    \n",
    "    # Calculate initial losses before any training\n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initial training loss\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        # Initial test loss\n",
    "        for x_t, x_t_plus_1 in test_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    # Add initial losses (pre-training)\n",
    "    initial_train_loss = total_train_loss * batch_size_train / train_size\n",
    "    initial_test_loss = total_test_loss * batch_size_test / test_size\n",
    "    train_losses.append(initial_train_loss)\n",
    "    test_losses.append(initial_test_loss)\n",
    "    \n",
    "    print(f\"Initial training loss: {initial_train_loss:.6f}\")\n",
    "    print(f\"Initial test loss: {initial_test_loss:.6f}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_t_plus_1 in test_dataloader:\n",
    "                x_t = x_t.to(device)\n",
    "                x_t_plus_1 = x_t_plus_1.to(device)\n",
    "                \n",
    "                predicted = model(x_t)\n",
    "                test_loss = criterion(predicted, x_t_plus_1)\n",
    "                total_test_loss += test_loss.item()\n",
    "        \n",
    "        # Normalize losses by total dataset size instead of number of batches\n",
    "        avg_train_loss = total_train_loss * batch_size_train / train_size\n",
    "        avg_test_loss = total_test_loss * batch_size_test / test_size\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.6f}\")\n",
    "            print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Training complete{run_str}!\")\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "def simulate_state_evolution(model, M, x0, num_timesteps, F, f, device='cpu'):\n",
    "    \"\"\"\n",
    "    Simulates state evolution using both the trained model and true model M.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained neural network\n",
    "        M: True model matrix\n",
    "        x0: Initial state\n",
    "        num_timesteps: Number of timesteps to simulate\n",
    "        F: Forcing parameter\n",
    "        f: Forcing field\n",
    "        device: Device to run model on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (neural_net_states, true_model_states)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x0_tensor = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "    f_tensor = torch.tensor(f, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Lists to store states over time\n",
    "    neural_net_states = [x0]\n",
    "    true_model_states = [x0]\n",
    "    \n",
    "    # Simulate using neural network\n",
    "    with torch.no_grad():\n",
    "        current_state = x0_tensor\n",
    "        for t in range(num_timesteps):\n",
    "            next_state = model(current_state.unsqueeze(0)).squeeze(0)\n",
    "            neural_net_states.append(next_state.cpu().numpy())\n",
    "            current_state = next_state\n",
    "    \n",
    "    # Simulate using true model\n",
    "    current_state = x0\n",
    "    for t in range(num_timesteps):\n",
    "        next_state = M @ current_state + F * f\n",
    "        true_model_states.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return neural_net_states, true_model_states\n",
    "\n",
    "def train_multiple_models(\n",
    "    nr: int, \n",
    "    nc: int, \n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    train_data: dict,\n",
    "    num_runs: int = 10,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    device: str = 'cpu',\n",
    "    num_timesteps_sim: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multiple models and compare their simulation results with the true model.\n",
    "    Displays 5 evenly spaced timesteps in the visualization.\n",
    "    \n",
    "    Args:\n",
    "        nr: Number of rows in the grid\n",
    "        nc: Number of columns in the grid\n",
    "        train_dataloader: DataLoader for training data\n",
    "        test_dataloader: DataLoader for test data\n",
    "        train_data: Dictionary containing model matrix, metadata, and shared parameters\n",
    "        num_runs: Number of models to train\n",
    "        num_epochs: Number of epochs per training run\n",
    "        learning_rate: Learning rate for optimization\n",
    "        device: Device to run training on\n",
    "        num_timesteps_sim: Number of timesteps to simulate\n",
    "    \"\"\"\n",
    "    all_models = []\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    \n",
    "    # Extract simulation parameters from train_data\n",
    "    M = train_data['model_matrix']\n",
    "    metadata = train_data['metadata']\n",
    "    f = train_data['shared_params']['f'].numpy()\n",
    "    \n",
    "    # Calculate the indices for 5 evenly spaced timesteps\n",
    "    display_timesteps = [\n",
    "        0,  # First timestep\n",
    "        num_timesteps_sim // 4,  # Quarter way\n",
    "        num_timesteps_sim // 2,  # Halfway\n",
    "        3 * num_timesteps_sim // 4,  # Three-quarters way\n",
    "        num_timesteps_sim  # Last timestep\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\nStarting model {i+1}/{num_runs}\")\n",
    "        model = StateModelNet(nr, nc)\n",
    "        \n",
    "        trained_model, train_losses, test_losses = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            device=device,\n",
    "            run_number=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot individual run results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(0, num_epochs + 1)  # Start from 0 to include initial loss\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "        plt.title(f'Training and Testing Loss Over Time - Run {i+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # After training, compare model simulations\n",
    "        # Get sample initial state from test data\n",
    "        sample_batch = next(iter(test_dataloader))\n",
    "        x0 = sample_batch[0][0].numpy()  # Take first state from first batch\n",
    "        \n",
    "        # Simulate evolution using both models\n",
    "        neural_net_states, true_model_states = simulate_state_evolution(\n",
    "            trained_model, M, x0, num_timesteps=num_timesteps_sim, F=metadata['F'], f=f, device=device\n",
    "        )\n",
    "        \n",
    "        # Extract only the desired timesteps for visualization\n",
    "        neural_net_display = [neural_net_states[t] for t in display_timesteps]\n",
    "        true_model_display = [true_model_states[t] for t in display_timesteps]\n",
    "        \n",
    "        # Plot comparison\n",
    "        helper.plot_multi_heatmap_time_evolution(\n",
    "            saved_timesteps=display_timesteps,  # Use the display timesteps\n",
    "            many_states_over_time=[neural_net_display, true_model_display],\n",
    "            nr=nr,\n",
    "            nc=nc,\n",
    "            titles=[\"Neural Network\", \"True Model\"],\n",
    "            big_title=f\"State Evolution Comparison - Model {i+1}\\nTimesteps {display_timesteps}\",\n",
    "            vmin=None,\n",
    "            vmax=None\n",
    "        )\n",
    "        \n",
    "        all_models.append(trained_model)\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_test_losses.append(test_losses)\n",
    "    \n",
    "    return all_models, all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset('state_transition_data_20000_2000.h5')\n",
    "train_loader = DataLoader(SavedDataset(train_data['train']), batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(SavedDataset(test_data['test']), batch_size=1000, shuffle=False)\n",
    "\n",
    "M = train_data['model_matrix']\n",
    "metadata = train_data['metadata']\n",
    "x0 = torch.randn(metadata['nr'] * metadata['nc'])\n",
    "\n",
    "# Train multiple models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "models, train_losses, test_losses = train_multiple_models(\n",
    "    nr=10, \n",
    "    nc=10, \n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    train_data=train_data,  # Pass in the train_data dictionary\n",
    "    num_runs=1,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    num_timesteps_sim=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simulate Using Model (DONE)\n",
    "- Include current parameters: describe how they work, how many weights we have vs parameters (DONE)\n",
    "- Try 100 hidden units (DONE)\n",
    "- Regularization\n",
    "- Try purely linear network\n",
    "- Make ReLU clearer\n",
    "- How much smaller can we make it? - Vince\n",
    "\n",
    "Previous problems: \n",
    "* Accidentally created different model for testing and training\n",
    "* Parameters were too big, caused nasty divergence (heat temps exploding way too high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
