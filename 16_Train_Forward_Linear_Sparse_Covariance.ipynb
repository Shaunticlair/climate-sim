{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Goal\n",
    "\n",
    "Our model is designed to compute:\n",
    "\n",
    "$x_{t+1}$ from $x_t$\n",
    "\n",
    "So we can directly predict the next state of our system given the current state.\n",
    "\n",
    "Unlike the base model, we will use a single set of fixed parameters for the entire dataset, rather than generating new parameters for each datapoint.\n",
    "\n",
    "# Our Data\n",
    "\n",
    "Our model receives:\n",
    "- $x_t$: Current state\n",
    "\n",
    "And attempts to predict:\n",
    "- $x_{t+1}$: The next state\n",
    "\n",
    "We generate our data by:\n",
    "- First establishing fixed physical parameters for the entire dataset\n",
    "- Then for each datapoint:\n",
    "  - Generating a new, random initial state $x_t$\n",
    "  - Running it forward one timestep to get $x_{t+1}$\n",
    "  - Using this pair to create one datapoint\n",
    "  - Repeat with new random initial states but same fixed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelDataset(Dataset):\n",
    "    def __init__(self, num_runs: int, timesteps_per_run: int, nr: int, nc: int, dt: float, F: float, fixed_params: dict = None):\n",
    "        \"\"\"\n",
    "        Initialize dataset with fixed parameters for all samples.\n",
    "        \n",
    "        Args:\n",
    "            num_runs (int): Number of different world configurations to generate\n",
    "            timesteps_per_run (int): Number of timesteps to simulate for each world\n",
    "            nr (int): Number of rows in grid\n",
    "            nc (int): Number of columns in grid\n",
    "            dt (float): Time step size\n",
    "            F (float): Forcing parameter\n",
    "            fixed_params (dict): Optional pre-generated parameters to use instead of generating new ones\n",
    "        \"\"\"\n",
    "        self.num_runs = num_runs\n",
    "        self.timesteps_per_run = timesteps_per_run\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        self.dt = dt\n",
    "        self.F = F\n",
    "        \n",
    "        # Either use provided parameters or generate new ones\n",
    "        self.fixed_params = fixed_params if fixed_params is not None else self._generate_params()\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def _generate_gaussian_field(self, n, nrv, ncv):\n",
    "        \"\"\"\n",
    "        Generate a Gaussian field composed of n Gaussian functions.\n",
    "        Uses same parameters as helper.py for consistency.\n",
    "        \"\"\"\n",
    "        mux = np.random.choice(ncv, n)\n",
    "        muy = np.random.choice(range(2, nrv - 2), n)\n",
    "        sigmax = np.random.uniform(1, ncv/4, n)\n",
    "        sigmay = np.random.uniform(1, nrv/4, n)\n",
    "\n",
    "        v = np.zeros((nrv, ncv))\n",
    "        for i in range(n):\n",
    "            for x in range(ncv):\n",
    "                for y in range(nrv):\n",
    "                    # Create three copies for pseudo-periodic field\n",
    "                    gauss = np.exp(-((x-mux[i])**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]-ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]+ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    v[y,x] += gauss\n",
    "        return v\n",
    "\n",
    "    def _generate_circular_field(self, v):\n",
    "        \"\"\"Generate a circular field from gradient of input field.\"\"\"\n",
    "        grad_v_y, grad_v_x = np.gradient(v)\n",
    "        return -grad_v_y, grad_v_x\n",
    "\n",
    "    def _generate_params(self):\n",
    "        \"\"\"\n",
    "        Generate fixed model parameters to be used for all samples.\n",
    "        Modified to match helper.py parameter generation.\n",
    "        \"\"\"\n",
    "        # Base grid parameters - unit spacing\n",
    "        DX_C = torch.ones(self.nr, self.nc + 1)\n",
    "        DY_C = torch.ones(self.nr + 1, self.nc)\n",
    "        DX_G = torch.ones(self.nr + 1, self.nc)\n",
    "        DY_G = torch.ones(self.nr, self.nc + 1)\n",
    "        RAC = torch.ones(self.nr, self.nc)\n",
    "\n",
    "        # Generate random diffusivities (must be positive)\n",
    "        # Using random uniform distribution between 0 and 1\n",
    "        KX = torch.rand(self.nr, self.nc + 1)\n",
    "        KY = torch.rand(self.nr + 1, self.nc)\n",
    "\n",
    "        # Generate velocities using Gaussian field approach\n",
    "        num_gauss = 8  # Number of Gaussian functions for velocity field\n",
    "        gauss = self._generate_gaussian_field(num_gauss, self.nr + 1, self.nc + 1)\n",
    "        VX_np, VY_np = self._generate_circular_field(gauss)\n",
    "        \n",
    "        # Convert velocities to PyTorch and scale\n",
    "        # Using scaling factor of 100 as in helper.py\n",
    "        VX = torch.from_numpy(100 * VX_np[:-1, :]).float()\n",
    "        VY = torch.from_numpy(100 * VY_np[:, :-1]).float()\n",
    "\n",
    "        # Generate random forcing field with controlled magnitude\n",
    "        # Using standard normal distribution scaled down\n",
    "        cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = self.nr, nc = self.nc, std_dev = 1.5)\n",
    "        f = helper.generate_random_vector_mean_0_cov_C( nr = self.nr, nc = self.nc, C = cov_C )[1]\n",
    "        \n",
    "        # Torchify and scale\n",
    "        f = torch.from_numpy(1 * f).float().reshape(-1)\n",
    "        #print(f.shape)\n",
    "\n",
    "        return {\n",
    "            'KX': KX,\n",
    "            'KY': KY,\n",
    "            'DX_C': DX_C,\n",
    "            'DY_C': DY_C,\n",
    "            'DX_G': DX_G,\n",
    "            'DY_G': DY_G,\n",
    "            'VX': VX,\n",
    "            'VY': VY,\n",
    "            'RAC': RAC,\n",
    "            'f': f,\n",
    "        }\n",
    "\n",
    "    def _generate_samples(self):\n",
    "        \"\"\"Generate multiple samples using fixed parameters but with controlled initial states.\"\"\"\n",
    "        samples = []\n",
    "        total_samples = self.num_runs * (self.timesteps_per_run - 1)  # -1 because we need pairs\n",
    "\n",
    "        cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = self.nr, nc = self.nc, std_dev = 1.5)\n",
    "        \n",
    "        for run in range(self.num_runs):\n",
    "            if run % 100 == 0:\n",
    "                print(f\"Generating world {run+1}/{self.num_runs}\")\n",
    "            \n",
    "            # Generate initial state with controlled magnitude\n",
    "            # Scale by 1/sqrt(n) to maintain reasonable magnitudes\n",
    "            x_current = helper.generate_random_vector_mean_0_cov_C( nr = self.nr, nc = self.nc, C = cov_C )[1]\n",
    "            \n",
    "            # Torchify and scale\n",
    "            x_current = torch.from_numpy(1 * x_current).float().reshape(-1)\n",
    "            #print(x_current.shape)\n",
    "            \n",
    "            # Generate timesteps for this world\n",
    "            world_states = [x_current]\n",
    "            for t in range(self.timesteps_per_run - 1):\n",
    "                x_next = self._compute_next_state(x_current)\n",
    "                world_states.append(x_next)\n",
    "                x_current = x_next\n",
    "            \n",
    "            # Create pairs of consecutive states as samples\n",
    "            for t in range(len(world_states) - 1):\n",
    "                samples.append((world_states[t], world_states[t + 1]))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def _compute_next_state(self, x_t):\n",
    "        \"\"\"Compute x(t+1) using fixed parameters.\"\"\"\n",
    "        # Convert PyTorch tensors to numpy arrays for helper function\n",
    "        np_params = {\n",
    "            key: tensor.cpu().detach().numpy() if torch.is_tensor(tensor) else tensor\n",
    "            for key, tensor in self.fixed_params.items()\n",
    "        }\n",
    "        \n",
    "        # Get model matrix M using helper function\n",
    "        M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "            nr=self.nr,\n",
    "            nc=self.nc,\n",
    "            dt=self.dt,\n",
    "            KX=np_params['KX'],\n",
    "            KY=np_params['KY'],\n",
    "            DX_C=np_params['DX_C'],\n",
    "            DY_C=np_params['DY_C'], \n",
    "            DX_G=np_params['DX_G'],\n",
    "            DY_G=np_params['DY_G'],\n",
    "            VX=np_params['VX'],\n",
    "            VY=np_params['VY'],\n",
    "            RAC=np_params['RAC'],\n",
    "            F=self.F,\n",
    "            cyclic_east_west=True,\n",
    "            cyclic_north_south=False,\n",
    "            M_is_sparse=False\n",
    "        )\n",
    "        \n",
    "        x_t_np = x_t.cpu().detach().numpy()\n",
    "        f_np = np_params['f']\n",
    "        \n",
    "        result_np = M @ x_t_np + self.F * f_np\n",
    "        return torch.from_numpy(result_np).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# To test _generate_samples, we will generate a sample x using the covariance procedure\\n\\n# Generate a sample x\\nnr = 10\\nnc = 10\\ncov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\\nz,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\\nx = torch.from_numpy(1 * x).float()\\n\\n# Plot x to visualize\\nplt.imshow(x.reshape(nr, nc))\\nplt.colorbar()\\nplt.title(\"Sample x\")\\nplt.show()\\n\\n# Plot z to visualize\\nplt.imshow(z.reshape(nr, nc))\\nplt.colorbar()\\nplt.title(\"Sample z\")\\nplt.show()\\n\\n\\n# To test _compute_next_state, we will plot the evolution of a sample x\\nnr = 64\\nnc = 64\\ncov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\\nz,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\\n\\n# Generate fixed parameters\\nnum_runs = 1\\ntimesteps_per_run = 2\\ndt = 0.01\\nF = 0.01\\ndataset = StateModelDataset(num_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=None)\\n\\n# Generate initial state\\nx = x_current = torch.from_numpy(1 * x).float().reshape(-1)\\n\\n# Generate next states\\nworld_states = [x]\\nfor t in range(200 - 1):\\n    print(t)\\n    x_next = dataset._compute_next_state(x)\\n    world_states.append(x_next)\\n    x = x_next\\n\\n# Plot evolution of x\\n#plot_2d_heatmap_time_evolution(saved_timesteps, state_over_time, nr, nc, vmin = None, vmax = None)\\n\\n#Convert world_states to 1D numpy array\\nworld_states = [x.reshape(nr*nc).cpu().detach().numpy() for x in world_states]\\n# We only want to plot the 10i+1 timesteps\\nworld_states = world_states[::20]\\n\\n\\n\\nhelper.plot_2d_heatmap_time_evolution( [20*i+1 for i in range(len(world_states))], world_states, nr, nc, vmin = None, vmax = None)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# To test _generate_samples, we will generate a sample x using the covariance procedure\n",
    "\n",
    "# Generate a sample x\n",
    "nr = 10\n",
    "nc = 10\n",
    "cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\n",
    "z,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\n",
    "x = torch.from_numpy(1 * x).float()\n",
    "\n",
    "# Plot x to visualize\n",
    "plt.imshow(x.reshape(nr, nc))\n",
    "plt.colorbar()\n",
    "plt.title(\"Sample x\")\n",
    "plt.show()\n",
    "\n",
    "# Plot z to visualize\n",
    "plt.imshow(z.reshape(nr, nc))\n",
    "plt.colorbar()\n",
    "plt.title(\"Sample z\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# To test _compute_next_state, we will plot the evolution of a sample x\n",
    "nr = 64\n",
    "nc = 64\n",
    "cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\n",
    "z,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\n",
    "\n",
    "# Generate fixed parameters\n",
    "num_runs = 1\n",
    "timesteps_per_run = 2\n",
    "dt = 0.01\n",
    "F = 0.01\n",
    "dataset = StateModelDataset(num_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=None)\n",
    "\n",
    "# Generate initial state\n",
    "x = x_current = torch.from_numpy(1 * x).float().reshape(-1)\n",
    "\n",
    "# Generate next states\n",
    "world_states = [x]\n",
    "for t in range(200 - 1):\n",
    "    print(t)\n",
    "    x_next = dataset._compute_next_state(x)\n",
    "    world_states.append(x_next)\n",
    "    x = x_next\n",
    "\n",
    "# Plot evolution of x\n",
    "#plot_2d_heatmap_time_evolution(saved_timesteps, state_over_time, nr, nc, vmin = None, vmax = None)\n",
    "\n",
    "#Convert world_states to 1D numpy array\n",
    "world_states = [x.reshape(nr*nc).cpu().detach().numpy() for x in world_states]\n",
    "# We only want to plot the 10i+1 timesteps\n",
    "world_states = world_states[::20]\n",
    "\n",
    "\n",
    "\n",
    "helper.plot_2d_heatmap_time_evolution( [20*i+1 for i in range(len(world_states))], world_states, nr, nc, vmin = None, vmax = None)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelNet(torch.nn.Module):\n",
    "    def __init__(self, nr: int, nc: int):\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size  # Now only takes state as input\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        # Original simple architecture with two layers\n",
    "\n",
    "        hidden_size = 2048 #self.output_size \n",
    "        self.fc1 = torch.nn.Linear(self.input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, self.output_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Ensure x has correct shape\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PrunedStateModelNet(nn.Module):\n",
    "    def __init__(self, nr: int, nc: int, M: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize network using sparse matrix operations for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            nr: Number of rows in grid\n",
    "            nc: Number of columns in grid\n",
    "            M: The true model matrix to base sparsity pattern on\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        # Convert M matrix to sparse format\n",
    "        M_tensor = torch.from_numpy(M).float()\n",
    "        indices = torch.nonzero(M_tensor).t()  # 2xN tensor of indices\n",
    "        values = torch.ones(indices.size(1))   # Initialize weights to 1.0\n",
    "        \n",
    "        # Create sparse weight matrix\n",
    "        self.weight = nn.Parameter(values)\n",
    "        self.register_buffer('indices', indices)\n",
    "        \n",
    "        # Store shape for sparse matrix construction\n",
    "        self.matrix_shape = (self.state_size, self.state_size)\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        self._sparsity = 1 - (indices.size(1) / (self.state_size * self.state_size))\n",
    "        print(f\"Created pruned model with {self._sparsity*100:.2f}% sparsity\")\n",
    "        print(f\"Original parameters: {self.state_size * self.state_size}\")\n",
    "        print(f\"Non-zero parameters: {indices.size(1)}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Construct sparse weight matrix\n",
    "        weight_matrix = torch.sparse_coo_tensor(\n",
    "            self.indices, \n",
    "            self.weight,\n",
    "            self.matrix_shape\n",
    "        )\n",
    "        \n",
    "        # Perform sparse matrix multiplication\n",
    "        return torch.sparse.mm(x, weight_matrix.t())\n",
    "    \n",
    "    @property\n",
    "    def sparsity(self) -> float:\n",
    "        \"\"\"Calculate the sparsity of the network (percentage of pruned weights).\"\"\"\n",
    "        return float(self._sparsity)\n",
    "    \n",
    "    def get_dense_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Return the weight matrix in dense format (for analysis/comparison).\"\"\"\n",
    "        dense = torch.zeros(self.matrix_shape, device=self.weight.device)\n",
    "        dense[self.indices[0], self.indices[1]] = self.weight\n",
    "        return dense\n",
    "\n",
    "def create_pruned_model(nr: int, nc: int, M: np.ndarray) -> PrunedStateModelNet:\n",
    "    \"\"\"\n",
    "    Create an efficiently sparse model that only stores and computes with non-zero weights.\n",
    "    \n",
    "    Args:\n",
    "        nr: Number of rows in grid\n",
    "        nc: Number of columns in grid\n",
    "        M: The true model matrix to base sparsity pattern on\n",
    "        \n",
    "    Returns:\n",
    "        Initialized PrunedStateModelNet\n",
    "    \"\"\"\n",
    "    M_tensor = torch.from_numpy(M).float()\n",
    "    model = PrunedStateModelNet(nr, nc, M)\n",
    "    \n",
    "    # Print sparsity information\n",
    "    print(f\"Created pruned model with {model.sparsity*100:.2f}% sparsity\")\n",
    "    print(f\"Original parameters: {nr * nc * nr * nc}\")\n",
    "    print(f\"Non-zero parameters: {torch.sum(model.weight != 0).item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_dataset(\n",
    "    filename: str,\n",
    "    num_train_runs: int,\n",
    "    num_test_runs: int,\n",
    "    timesteps_per_run: int,\n",
    "    nr: int,\n",
    "    nc: int,\n",
    "    dt: float,\n",
    "    F: float\n",
    ") -> None:\n",
    "    # Generate a single set of parameters first\n",
    "    temp_dataset = StateModelDataset(1, 1, nr, nc, dt, F)\n",
    "    shared_params = temp_dataset.fixed_params\n",
    "    \n",
    "    # Generate the model matrix M using the helper function\n",
    "    M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "        nr=nr,\n",
    "        nc=nc,\n",
    "        dt=dt,\n",
    "        KX=shared_params['KX'].numpy(),\n",
    "        KY=shared_params['KY'].numpy(),\n",
    "        DX_C=shared_params['DX_C'].numpy(),\n",
    "        DY_C=shared_params['DY_C'].numpy(),\n",
    "        DX_G=shared_params['DX_G'].numpy(),\n",
    "        DY_G=shared_params['DY_G'].numpy(),\n",
    "        VX=shared_params['VX'].numpy(),\n",
    "        VY=shared_params['VY'].numpy(),\n",
    "        RAC=shared_params['RAC'].numpy(),\n",
    "        F=F,\n",
    "        cyclic_east_west=True,\n",
    "        cyclic_north_south=False,\n",
    "        M_is_sparse=False\n",
    "    )\n",
    "    \n",
    "    # Create dataset instances with shared parameters\n",
    "    train_dataset = StateModelDataset(num_train_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    test_dataset = StateModelDataset(num_test_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Save top-level data that's shared between train and test\n",
    "        f.create_dataset('model_matrix', data=M)  # Store M at top level\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['nr'] = nr\n",
    "        f.attrs['nc'] = nc\n",
    "        f.attrs['dt'] = dt\n",
    "        f.attrs['F'] = F\n",
    "        f.attrs['timesteps_per_run'] = timesteps_per_run\n",
    "        \n",
    "        # Save the shared parameters\n",
    "        params_group = f.create_group('shared_params')\n",
    "        for key, value in shared_params.items():\n",
    "            params_group.create_dataset(key, data=value.numpy())\n",
    "        \n",
    "        # Create train and test groups for the actual datasets\n",
    "        train_group = f.create_group('train')\n",
    "        test_group = f.create_group('test')\n",
    "        \n",
    "        # Helper function to save a single dataset\n",
    "        def save_dataset(group, dataset, desc):\n",
    "            samples_group = group.create_group('samples')\n",
    "            for i, (x_t, x_t_plus_1) in enumerate(tqdm(dataset, desc=desc)):\n",
    "                sample_group = samples_group.create_group(f'sample_{i}')\n",
    "                sample_group.create_dataset('x_t', data=x_t.numpy())\n",
    "                sample_group.create_dataset('x_t_plus_1', data=x_t_plus_1.numpy())\n",
    "        \n",
    "        # Save training and testing datasets\n",
    "        save_dataset(train_group, train_dataset, \"Saving training data\")\n",
    "        save_dataset(test_group, test_dataset, \"Saving testing data\")\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple[Dict, Dict]:\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Load top-level shared data\n",
    "        metadata = {\n",
    "            'nr': f.attrs['nr'],\n",
    "            'nc': f.attrs['nc'],\n",
    "            'dt': f.attrs['dt'],\n",
    "            'F': f.attrs['F'],\n",
    "            'timesteps_per_run': f.attrs['timesteps_per_run']\n",
    "        }\n",
    "        \n",
    "        # Load the model matrix M from top level\n",
    "        M = f['model_matrix'][:]\n",
    "        \n",
    "        # Load shared parameters\n",
    "        shared_params = {}\n",
    "        params_group = f['shared_params']\n",
    "        for key in params_group.keys():\n",
    "            shared_params[key] = torch.from_numpy(params_group[key][:]).float()\n",
    "        \n",
    "        def load_dataset(group):\n",
    "            data = []\n",
    "            samples_group = group['samples']\n",
    "            for sample_name in tqdm(samples_group.keys(), desc=f\"Loading {group.name} data\"):\n",
    "                sample = samples_group[sample_name]\n",
    "                x_t = torch.from_numpy(sample['x_t'][:]).float()\n",
    "                x_t_plus_1 = torch.from_numpy(sample['x_t_plus_1'][:]).float()\n",
    "                data.append((x_t, x_t_plus_1))\n",
    "            return data\n",
    "        \n",
    "        train_data = load_dataset(f['train'])\n",
    "        test_data = load_dataset(f['test'])\n",
    "    \n",
    "    # Create common shared data dictionary\n",
    "    shared_data = {\n",
    "        'metadata': metadata,\n",
    "        'shared_params': shared_params,\n",
    "        'model_matrix': M\n",
    "    }\n",
    "    \n",
    "    # Return dictionaries that combine shared data with specific train/test data\n",
    "    return {\n",
    "        'train': train_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }, {\n",
    "        'test': test_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }\n",
    "\n",
    "\n",
    "class SavedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pre-saved data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def create_dataloaders(train_data, test_data, batch_size=32, shuffle_train=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader objects for training and testing datasets.\n",
    "    Now handles data without parameters.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training dataset dictionary containing 'train' data\n",
    "        test_data: Testing dataset dictionary containing 'test' data\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        shuffle_train: Whether to shuffle training data\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoader objects\n",
    "    \"\"\"\n",
    "    train_dataset = SavedDataset(train_data['train'])\n",
    "    test_dataset = SavedDataset(test_data['test'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset with 180000 training samples and 18000 testing samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ngenerate_and_save_dataset(\\n    filename=filename,\\n    num_train_runs=num_train_runs,      # Number of different runs for training\\n    num_test_runs=num_test_runs,        # Number of different runs for testing\\n    timesteps_per_run=10,      # Number of timesteps per run\\n    nr=nr,\\n    nc=nc,\\n    dt=0.01,\\n    F=0.01\\n)\\n\\n# Example usage\\n\\n\\n# Load dataset and create dataloaders\\ntrain_data, test_data = load_dataset('state_model_data_one-world.h5')\\ntrain_loader, test_loader = create_dataloaders(\\n    train_data, \\n    test_data, \\n    batch_size=32\\n)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Generate and save datasets\n",
    "nr,nc = 64,64\n",
    "num_train_runs = 20000\n",
    "num_test_runs = 2000\n",
    "timesteps_per_run = 10\n",
    "num_train_data = num_train_runs * (timesteps_per_run - 1)\n",
    "num_test_data = num_test_runs * (timesteps_per_run - 1)\n",
    "print(f\"Generating dataset with {num_train_data} training samples and {num_test_data} testing samples\")\n",
    "filename = f'data_forward-sim_cov_{nr}x{nc}_train{num_train_data}_test{num_test_data}.h5'\n",
    "#filename = f'data_forward-sim_{nr}x{nc}_train{num_train_data}_test{num_test_data}.h5'\n",
    "\"\"\"\n",
    "generate_and_save_dataset(\n",
    "    filename=filename,\n",
    "    num_train_runs=num_train_runs,      # Number of different runs for training\n",
    "    num_test_runs=num_test_runs,        # Number of different runs for testing\n",
    "    timesteps_per_run=10,      # Number of timesteps per run\n",
    "    nr=nr,\n",
    "    nc=nc,\n",
    "    dt=0.01,\n",
    "    F=0.01\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Load dataset and create dataloaders\n",
    "train_data, test_data = load_dataset('state_model_data_one-world.h5')\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    train_data, \n",
    "    test_data, \n",
    "    batch_size=32\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: str = 'cpu',\n",
    "    run_number: int = None\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Get total dataset sizes\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    batch_size_train = train_dataloader.batch_size\n",
    "    batch_size_test = test_dataloader.batch_size\n",
    "    \n",
    "    run_str = f\" (Run {run_number})\" if run_number is not None else \"\"\n",
    "    print(f\"Training started{run_str}...\")\n",
    "    \n",
    "    # Calculate initial losses before any training\n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initial training loss\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        # Initial test loss\n",
    "        for x_t, x_t_plus_1 in test_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    # Add initial losses (pre-training)\n",
    "    initial_train_loss = total_train_loss * batch_size_train / train_size\n",
    "    initial_test_loss = total_test_loss * batch_size_test / test_size\n",
    "    train_losses.append(initial_train_loss)\n",
    "    test_losses.append(initial_test_loss)\n",
    "    \n",
    "    print(f\"Initial training loss: {initial_train_loss:.6f}\")\n",
    "    print(f\"Initial test loss: {initial_test_loss:.6f}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_t_plus_1 in test_dataloader:\n",
    "                x_t = x_t.to(device)\n",
    "                x_t_plus_1 = x_t_plus_1.to(device)\n",
    "                \n",
    "                predicted = model(x_t)\n",
    "                test_loss = criterion(predicted, x_t_plus_1)\n",
    "                total_test_loss += test_loss.item()\n",
    "        \n",
    "        # Normalize losses by total dataset size instead of number of batches\n",
    "        avg_train_loss = total_train_loss * batch_size_train / train_size\n",
    "        avg_test_loss = total_test_loss * batch_size_test / test_size\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.6f}\")\n",
    "            print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Training complete{run_str}!\")\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "def simulate_state_evolution(model, M, x0, num_timesteps, F, f, device='cpu'):\n",
    "    \"\"\"\n",
    "    Simulates state evolution using both the trained model and true model M.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained neural network\n",
    "        M: True model matrix\n",
    "        x0: Initial state\n",
    "        num_timesteps: Number of timesteps to simulate\n",
    "        F: Forcing parameter\n",
    "        f: Forcing field\n",
    "        device: Device to run model on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (neural_net_states, true_model_states)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x0_tensor = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "    f_tensor = torch.tensor(f, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Lists to store states over time\n",
    "    neural_net_states = [x0]\n",
    "    true_model_states = [x0]\n",
    "    \n",
    "    # Simulate using neural network\n",
    "    with torch.no_grad():\n",
    "        current_state = x0_tensor\n",
    "        for t in range(num_timesteps):\n",
    "            next_state = model(current_state.unsqueeze(0)).squeeze(0)\n",
    "            neural_net_states.append(next_state.cpu().numpy())\n",
    "            current_state = next_state\n",
    "    \n",
    "    # Simulate using true model\n",
    "    current_state = x0\n",
    "    for t in range(num_timesteps):\n",
    "        next_state = M @ current_state + F * f\n",
    "        true_model_states.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return neural_net_states, true_model_states\n",
    "\n",
    "def train_multiple_models(\n",
    "    nr: int, \n",
    "    nc: int, \n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    train_data: dict,\n",
    "    num_runs: int = 10,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    device: str = 'cpu',\n",
    "    num_timesteps_sim: int = 10,\n",
    "    prune = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multiple models and compare their simulation results with the true model.\n",
    "    Displays 5 evenly spaced timesteps in the visualization.\n",
    "    \n",
    "    Args:\n",
    "        nr: Number of rows in the grid\n",
    "        nc: Number of columns in the grid\n",
    "        train_dataloader: DataLoader for training data\n",
    "        test_dataloader: DataLoader for test data\n",
    "        train_data: Dictionary containing model matrix, metadata, and shared parameters\n",
    "        num_runs: Number of models to train\n",
    "        num_epochs: Number of epochs per training run\n",
    "        learning_rate: Learning rate for optimization\n",
    "        device: Device to run training on\n",
    "        num_timesteps_sim: Number of timesteps to simulate\n",
    "    \"\"\"\n",
    "    all_models = []\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    \n",
    "    # Extract simulation parameters from train_data\n",
    "    M = train_data['model_matrix']\n",
    "    metadata = train_data['metadata']\n",
    "    f = train_data['shared_params']['f'].numpy()\n",
    "    \n",
    "    # Calculate the indices for 5 evenly spaced timesteps\n",
    "    display_timesteps = [\n",
    "        0,  # First timestep\n",
    "        num_timesteps_sim // 4,  # Quarter way\n",
    "        num_timesteps_sim // 2,  # Halfway\n",
    "        3 * num_timesteps_sim // 4,  # Three-quarters way\n",
    "        num_timesteps_sim  # Last timestep\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\nStarting model {i+1}/{num_runs}\")\n",
    "        if prune:\n",
    "            model = create_pruned_model(nr, nc, M)\n",
    "        else:\n",
    "            model = StateModelNet(nr, nc)\n",
    "        \n",
    "        trained_model, train_losses, test_losses = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            device=device,\n",
    "            run_number=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot individual run results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(0, num_epochs + 1)  # Start from 0 to include initial loss\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "        plt.title(f'Training and Testing Loss Over Time - Run {i+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # After training, compare model simulations\n",
    "        # Get sample initial state from test data\n",
    "        sample_batch = next(iter(test_dataloader))\n",
    "        x0 = sample_batch[0][0].numpy()  # Take first state from first batch\n",
    "        \n",
    "        # Simulate evolution using both models\n",
    "        neural_net_states, true_model_states = simulate_state_evolution(\n",
    "            trained_model, M, x0, num_timesteps=num_timesteps_sim, F=metadata['F'], f=f, device=device\n",
    "        )\n",
    "        \n",
    "        # Extract only the desired timesteps for visualization\n",
    "        neural_net_display = [neural_net_states[t] for t in display_timesteps]\n",
    "        true_model_display = [true_model_states[t] for t in display_timesteps]\n",
    "        \n",
    "        # Plot comparison\n",
    "        helper.plot_multi_heatmap_time_evolution(\n",
    "            saved_timesteps=display_timesteps,  # Use the display timesteps\n",
    "            many_states_over_time=[neural_net_display, true_model_display],\n",
    "            nr=nr,\n",
    "            nc=nc,\n",
    "            titles=[\"Neural Network\", \"True Model\"],\n",
    "            big_title=f\"State Evolution Comparison - Model {i+1}\\nTimesteps {display_timesteps}\",\n",
    "            vmin=None,\n",
    "            vmax=None\n",
    "        )\n",
    "        \n",
    "        all_models.append(trained_model)\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_test_losses.append(test_losses)\n",
    "    \n",
    "    return all_models, all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /train data: 100%|██████████| 180000/180000 [04:05<00:00, 732.19it/s] \n",
      "Loading /test data: 100%|██████████| 18000/18000 [00:24<00:00, 731.22it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, test_data = load_dataset(filename)\n",
    "train_loader = DataLoader(SavedDataset(train_data['train']), batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(SavedDataset(test_data['test']), batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "M = train_data['model_matrix']\n",
    "metadata = train_data['metadata']\n",
    "nr, nc = metadata['nr'], metadata['nc']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Train multiple models\\ndevice = \\'cuda\\' if torch.cuda.is_available() else \\'cpu\\'\\nmodels, train_losses, test_losses = train_multiple_models(\\n    nr=nr,\\n    nc=nc,\\n    train_dataloader=train_loader,\\n    test_dataloader=test_loader,\\n    train_data=train_data,  # Pass in the train_data dictionary\\n    num_runs=1,\\n    num_epochs=100,\\n    device=device,\\n    num_timesteps_sim=100\\n)\\n\\n# Save models to disk\\nos.makedirs(\\'saved_models\\', exist_ok=True)\\n\\nfor i, model in enumerate(models):\\n    filename = f\\'saved_models/model_{i+1}.pt\\'\\n    torch.save(model.state_dict(), filename)\\n    print(f\"Model {i+1} saved to {filename}\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Train multiple models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "models, train_losses, test_losses = train_multiple_models(\n",
    "    nr=nr,\n",
    "    nc=nc,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    train_data=train_data,  # Pass in the train_data dictionary\n",
    "    num_runs=1,\n",
    "    num_epochs=100,\n",
    "    device=device,\n",
    "    num_timesteps_sim=100\n",
    ")\n",
    "\n",
    "# Save models to disk\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    filename = f'saved_models/model_{i+1}.pt'\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model {i+1} saved to {filename}\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we decompose $M$?\n",
    "\n",
    "Our goal is to write $M$ as \n",
    "\n",
    "$$M = LR^T$$\n",
    "\n",
    "Where $L$ and $R$ have dimension $(n \\times r)$, and $r << n$. This represents a sort of \"dimensionality reduction\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(M))\n",
    "\n",
    "# Our goal is decompose M = LR^T\n",
    "\n",
    "#print(np.linalg.cond(M))\n",
    "#Condition number of 3.66, so it is not ill-conditioned\n",
    "\n",
    "\n",
    "U, S, V = np.linalg.svd(M)\n",
    "#r = 10\n",
    "#L = U[:, :r] * np.sqrt(S[:r])  # Broadcasting S across columns\n",
    "#R = V[:r, :].T * np.sqrt(S[:r])  # Note V needs transpose and broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.67487833e-04 5.34947569e-04 8.02391235e-04 ... 9.99845435e-01\n",
      " 9.99926933e-01 1.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cf285c2c80>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cumulative_variance_ratio = np.cumsum(S) / np.sum(S)\n",
    "print(cumulative_variance_ratio)\n",
    "\n",
    "#Plot \n",
    "plt.plot(cumulative_variance_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Use the first input-output pair from the test data: not the whole batch, just the first data point\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sample_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "# Use the first input-output pair from the test data: not the whole batch, just the first data point\n",
    "sample_batch = next(iter(test_loader))\n",
    "input_batch, output_batch = sample_batch\n",
    "\n",
    "input_data = input_batch[0].unsqueeze(0)\n",
    "output_data = output_batch[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_gradients(model, input_data, output_data, M, nr=None, nc=None):\n",
    "    \"\"\"\n",
    "    Analyze the gradient matrix of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_data: Input tensor\n",
    "        output_data: Target output tensor\n",
    "        nr: Number of rows (for reshaping visualization)\n",
    "        nc: Number of columns (for reshaping visualization)\n",
    "    \"\"\"\n",
    "    # Compute Jacobian\n",
    "    jacobian = torch.autograd.functional.jacobian(model, input_data)\n",
    "    jacobian = jacobian.view(output_data.size(-1), input_data.size(-1))\n",
    "    jacobian = jacobian.detach().numpy()\n",
    "    \n",
    "    # For a linear model, extract the weight matrix for comparison\n",
    "    if not hasattr(model, 'fc2'):\n",
    "        \n",
    "        # Compute difference between Jacobian and weight matrix\n",
    "        diff = jacobian - M\n",
    "        max_diff = np.max(np.abs(diff))\n",
    "        print(f\"Maximum difference between Jacobian and weight matrix: {max_diff}\")\n",
    "        \n",
    "        if max_diff < 1e-6:\n",
    "            print(\"Gradient matrix is effectively identical to weight matrix (as expected for a linear model)\")\n",
    "        else:\n",
    "            print(\"Warning: Gradient matrix differs from weight matrix - model may not be purely linear\")\n",
    "    \n",
    "    # Visualize the gradient matrix\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(jacobian)\n",
    "    plt.colorbar()\n",
    "    plt.title('Gradient Matrix (Jacobian)')\n",
    "    \n",
    "    # If spatial dimensions are provided, show example input and output\n",
    "    if nr is not None and nc is not None:\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(input_data.detach().numpy().reshape(nr, nc))\n",
    "        plt.colorbar()\n",
    "        plt.title('Input State')\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.imshow(output_data.detach().numpy().reshape(nr, nc))\n",
    "        plt.colorbar()\n",
    "        plt.title('Target Output State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# Example usage:\n",
    "jacobian = analyze_gradients(model, input_data, output_data, M, nr=64, nc=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
