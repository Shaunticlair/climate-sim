{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Goal\n",
    "\n",
    "Our model is designed to compute:\n",
    "\n",
    "$x_{t+1}$ from $x_t$\n",
    "\n",
    "So we can directly predict the next state of our system given the current state.\n",
    "\n",
    "Unlike the base model, we will use a single set of fixed parameters for the entire dataset, rather than generating new parameters for each datapoint.\n",
    "\n",
    "# Our Data\n",
    "\n",
    "Our model receives:\n",
    "- $x_t$: Current state\n",
    "\n",
    "And attempts to predict:\n",
    "- $x_{t+1}$: The next state\n",
    "\n",
    "We generate our data by:\n",
    "- First establishing fixed physical parameters for the entire dataset\n",
    "- Then for each datapoint:\n",
    "  - Generating a new, random initial state $x_t$\n",
    "  - Running it forward one timestep to get $x_{t+1}$\n",
    "  - Using this pair to create one datapoint\n",
    "  - Repeat with new random initial states but same fixed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelDataset(Dataset):\n",
    "    def __init__(self, num_runs: int, timesteps_per_run: int, nr: int, nc: int, dt: float, F: float, fixed_params: dict = None):\n",
    "        \"\"\"\n",
    "        Initialize dataset with fixed parameters for all samples.\n",
    "        \n",
    "        Args:\n",
    "            num_runs (int): Number of different world configurations to generate\n",
    "            timesteps_per_run (int): Number of timesteps to simulate for each world\n",
    "            nr (int): Number of rows in grid\n",
    "            nc (int): Number of columns in grid\n",
    "            dt (float): Time step size\n",
    "            F (float): Forcing parameter\n",
    "            fixed_params (dict): Optional pre-generated parameters to use instead of generating new ones\n",
    "        \"\"\"\n",
    "        self.num_runs = num_runs\n",
    "        self.timesteps_per_run = timesteps_per_run\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        self.dt = dt\n",
    "        self.F = F\n",
    "        \n",
    "        # Either use provided parameters or generate new ones\n",
    "        self.fixed_params = fixed_params if fixed_params is not None else self._generate_params()\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def _generate_gaussian_field(self, n, nrv, ncv):\n",
    "        \"\"\"\n",
    "        Generate a Gaussian field composed of n Gaussian functions.\n",
    "        Uses same parameters as helper.py for consistency.\n",
    "        \"\"\"\n",
    "        mux = np.random.choice(ncv, n)\n",
    "        muy = np.random.choice(range(2, nrv - 2), n)\n",
    "        sigmax = np.random.uniform(1, ncv/4, n)\n",
    "        sigmay = np.random.uniform(1, nrv/4, n)\n",
    "\n",
    "        v = np.zeros((nrv, ncv))\n",
    "        for i in range(n):\n",
    "            for x in range(ncv):\n",
    "                for y in range(nrv):\n",
    "                    # Create three copies for pseudo-periodic field\n",
    "                    gauss = np.exp(-((x-mux[i])**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]-ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]+ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    v[y,x] += gauss\n",
    "        return v\n",
    "\n",
    "    def _generate_circular_field(self, v):\n",
    "        \"\"\"Generate a circular field from gradient of input field.\"\"\"\n",
    "        grad_v_y, grad_v_x = np.gradient(v)\n",
    "        return -grad_v_y, grad_v_x\n",
    "\n",
    "    def _generate_params(self):\n",
    "        \"\"\"\n",
    "        Generate fixed model parameters to be used for all samples.\n",
    "        Modified to match helper.py parameter generation.\n",
    "        \"\"\"\n",
    "        # Base grid parameters - unit spacing\n",
    "        DX_C = torch.ones(self.nr, self.nc + 1)\n",
    "        DY_C = torch.ones(self.nr + 1, self.nc)\n",
    "        DX_G = torch.ones(self.nr + 1, self.nc)\n",
    "        DY_G = torch.ones(self.nr, self.nc + 1)\n",
    "        RAC = torch.ones(self.nr, self.nc)\n",
    "\n",
    "        # Generate random diffusivities (must be positive)\n",
    "        # Using random uniform distribution between 0 and 1\n",
    "        KX = torch.rand(self.nr, self.nc + 1)\n",
    "        KY = torch.rand(self.nr + 1, self.nc)\n",
    "\n",
    "        # Generate velocities using Gaussian field approach\n",
    "        num_gauss = 8  # Number of Gaussian functions for velocity field\n",
    "        gauss = self._generate_gaussian_field(num_gauss, self.nr + 1, self.nc + 1)\n",
    "        VX_np, VY_np = self._generate_circular_field(gauss)\n",
    "        \n",
    "        # Convert velocities to PyTorch and scale\n",
    "        # Using scaling factor of 100 as in helper.py\n",
    "        VX = torch.from_numpy(100 * VX_np[:-1, :]).float()\n",
    "        VY = torch.from_numpy(100 * VY_np[:, :-1]).float()\n",
    "\n",
    "        # Generate random forcing field with controlled magnitude\n",
    "        # Using standard normal distribution scaled down\n",
    "        cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = self.nr, nc = self.nc, std_dev = 1.5)\n",
    "        f = helper.generate_random_vector_mean_0_cov_C( nr = self.nr, nc = self.nc, C = cov_C )[1]\n",
    "        \n",
    "        # Torchify and scale\n",
    "        f = torch.from_numpy(1 * f).float().reshape(-1)\n",
    "        #print(f.shape)\n",
    "\n",
    "        return {\n",
    "            'KX': KX,\n",
    "            'KY': KY,\n",
    "            'DX_C': DX_C,\n",
    "            'DY_C': DY_C,\n",
    "            'DX_G': DX_G,\n",
    "            'DY_G': DY_G,\n",
    "            'VX': VX,\n",
    "            'VY': VY,\n",
    "            'RAC': RAC,\n",
    "            'f': f,\n",
    "        }\n",
    "\n",
    "    def _generate_samples(self):\n",
    "        \"\"\"Generate multiple samples using fixed parameters but with controlled initial states.\"\"\"\n",
    "        samples = []\n",
    "        total_samples = self.num_runs * (self.timesteps_per_run - 1)  # -1 because we need pairs\n",
    "\n",
    "        cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = self.nr, nc = self.nc, std_dev = 1.5)\n",
    "        \n",
    "        for run in range(self.num_runs):\n",
    "            if run % 100 == 0:\n",
    "                print(f\"Generating world {run+1}/{self.num_runs}\")\n",
    "            \n",
    "            # Generate initial state with controlled magnitude\n",
    "            # Scale by 1/sqrt(n) to maintain reasonable magnitudes\n",
    "            x_current = helper.generate_random_vector_mean_0_cov_C( nr = self.nr, nc = self.nc, C = cov_C )[1]\n",
    "            \n",
    "            # Torchify and scale\n",
    "            x_current = torch.from_numpy(1 * x_current).float().reshape(-1)\n",
    "            #print(x_current.shape)\n",
    "            \n",
    "            # Generate timesteps for this world\n",
    "            world_states = [x_current]\n",
    "            for t in range(self.timesteps_per_run - 1):\n",
    "                x_next = self._compute_next_state(x_current)\n",
    "                world_states.append(x_next)\n",
    "                x_current = x_next\n",
    "            \n",
    "            # Create pairs of consecutive states as samples\n",
    "            for t in range(len(world_states) - 1):\n",
    "                samples.append((world_states[t], world_states[t + 1]))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def _compute_next_state(self, x_t):\n",
    "        \"\"\"Compute x(t+1) using fixed parameters.\"\"\"\n",
    "        # Convert PyTorch tensors to numpy arrays for helper function\n",
    "        np_params = {\n",
    "            key: tensor.cpu().detach().numpy() if torch.is_tensor(tensor) else tensor\n",
    "            for key, tensor in self.fixed_params.items()\n",
    "        }\n",
    "        \n",
    "        # Get model matrix M using helper function\n",
    "        M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "            nr=self.nr,\n",
    "            nc=self.nc,\n",
    "            dt=self.dt,\n",
    "            KX=np_params['KX'],\n",
    "            KY=np_params['KY'],\n",
    "            DX_C=np_params['DX_C'],\n",
    "            DY_C=np_params['DY_C'], \n",
    "            DX_G=np_params['DX_G'],\n",
    "            DY_G=np_params['DY_G'],\n",
    "            VX=np_params['VX'],\n",
    "            VY=np_params['VY'],\n",
    "            RAC=np_params['RAC'],\n",
    "            F=self.F,\n",
    "            cyclic_east_west=True,\n",
    "            cyclic_north_south=False,\n",
    "            M_is_sparse=False\n",
    "        )\n",
    "        \n",
    "        x_t_np = x_t.cpu().detach().numpy()\n",
    "        f_np = np_params['f']\n",
    "        \n",
    "        result_np = M @ x_t_np + self.F * f_np\n",
    "        return torch.from_numpy(result_np).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# To test _generate_samples, we will generate a sample x using the covariance procedure\\n\\n# Generate a sample x\\nnr = 10\\nnc = 10\\ncov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\\nz,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\\nx = torch.from_numpy(1 * x).float()\\n\\n# Plot x to visualize\\nplt.imshow(x.reshape(nr, nc))\\nplt.colorbar()\\nplt.title(\"Sample x\")\\nplt.show()\\n\\n# Plot z to visualize\\nplt.imshow(z.reshape(nr, nc))\\nplt.colorbar()\\nplt.title(\"Sample z\")\\nplt.show()\\n\\n\\n# To test _compute_next_state, we will plot the evolution of a sample x\\nnr = 64\\nnc = 64\\ncov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\\nz,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\\n\\n# Generate fixed parameters\\nnum_runs = 1\\ntimesteps_per_run = 2\\ndt = 0.01\\nF = 0.01\\ndataset = StateModelDataset(num_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=None)\\n\\n# Generate initial state\\nx = x_current = torch.from_numpy(1 * x).float().reshape(-1)\\n\\n# Generate next states\\nworld_states = [x]\\nfor t in range(200 - 1):\\n    print(t)\\n    x_next = dataset._compute_next_state(x)\\n    world_states.append(x_next)\\n    x = x_next\\n\\n# Plot evolution of x\\n#plot_2d_heatmap_time_evolution(saved_timesteps, state_over_time, nr, nc, vmin = None, vmax = None)\\n\\n#Convert world_states to 1D numpy array\\nworld_states = [x.reshape(nr*nc).cpu().detach().numpy() for x in world_states]\\n# We only want to plot the 10i+1 timesteps\\nworld_states = world_states[::20]\\n\\n\\n\\nhelper.plot_2d_heatmap_time_evolution( [20*i+1 for i in range(len(world_states))], world_states, nr, nc, vmin = None, vmax = None)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# To test _generate_samples, we will generate a sample x using the covariance procedure\n",
    "\n",
    "# Generate a sample x\n",
    "nr = 10\n",
    "nc = 10\n",
    "cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\n",
    "z,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\n",
    "x = torch.from_numpy(1 * x).float()\n",
    "\n",
    "# Plot x to visualize\n",
    "plt.imshow(x.reshape(nr, nc))\n",
    "plt.colorbar()\n",
    "plt.title(\"Sample x\")\n",
    "plt.show()\n",
    "\n",
    "# Plot z to visualize\n",
    "plt.imshow(z.reshape(nr, nc))\n",
    "plt.colorbar()\n",
    "plt.title(\"Sample z\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# To test _compute_next_state, we will plot the evolution of a sample x\n",
    "nr = 64\n",
    "nc = 64\n",
    "cov_C = helper.compute_covariance_matrix_gaussian_dropoff(nr = nr, nc = nc, std_dev = 1.5)\n",
    "z,x = helper.generate_random_vector_mean_0_cov_C( nr = nr, nc = nc, C = cov_C )\n",
    "\n",
    "# Generate fixed parameters\n",
    "num_runs = 1\n",
    "timesteps_per_run = 2\n",
    "dt = 0.01\n",
    "F = 0.01\n",
    "dataset = StateModelDataset(num_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=None)\n",
    "\n",
    "# Generate initial state\n",
    "x = x_current = torch.from_numpy(1 * x).float().reshape(-1)\n",
    "\n",
    "# Generate next states\n",
    "world_states = [x]\n",
    "for t in range(200 - 1):\n",
    "    print(t)\n",
    "    x_next = dataset._compute_next_state(x)\n",
    "    world_states.append(x_next)\n",
    "    x = x_next\n",
    "\n",
    "# Plot evolution of x\n",
    "#plot_2d_heatmap_time_evolution(saved_timesteps, state_over_time, nr, nc, vmin = None, vmax = None)\n",
    "\n",
    "#Convert world_states to 1D numpy array\n",
    "world_states = [x.reshape(nr*nc).cpu().detach().numpy() for x in world_states]\n",
    "# We only want to plot the 10i+1 timesteps\n",
    "world_states = world_states[::20]\n",
    "\n",
    "\n",
    "\n",
    "helper.plot_2d_heatmap_time_evolution( [20*i+1 for i in range(len(world_states))], world_states, nr, nc, vmin = None, vmax = None)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelNet(torch.nn.Module):\n",
    "    def __init__(self, nr: int, nc: int):\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size  # Now only takes state as input\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        # Original simple architecture with two layers\n",
    "\n",
    "        hidden_size = self.output_size #2048\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, hidden_size)\n",
    "        #self.fc2 = torch.nn.Linear(hidden_size, self.output_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Ensure x has correct shape\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.fc1(x)\n",
    "        #x = self.activation(x)\n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PrunedStateModelNet(nn.Module):\n",
    "    def __init__(self, nr: int, nc: int, M: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize network using sparse matrix operations for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            nr: Number of rows in grid\n",
    "            nc: Number of columns in grid\n",
    "            M: The true model matrix to base sparsity pattern on\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        # Convert M matrix to sparse format\n",
    "        M_tensor = torch.from_numpy(M).float()\n",
    "        indices = torch.nonzero(M_tensor).t()  # 2xN tensor of indices\n",
    "        values = torch.ones(indices.size(1))   # Initialize weights to 1.0\n",
    "        \n",
    "        # Create sparse weight matrix\n",
    "        self.weight = nn.Parameter(values)\n",
    "        self.register_buffer('indices', indices)\n",
    "        \n",
    "        # Store shape for sparse matrix construction\n",
    "        self.matrix_shape = (self.state_size, self.state_size)\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        self._sparsity = 1 - (indices.size(1) / (self.state_size * self.state_size))\n",
    "        print(f\"Created pruned model with {self._sparsity*100:.2f}% sparsity\")\n",
    "        print(f\"Original parameters: {self.state_size * self.state_size}\")\n",
    "        print(f\"Non-zero parameters: {indices.size(1)}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Construct sparse weight matrix\n",
    "        weight_matrix = torch.sparse_coo_tensor(\n",
    "            self.indices, \n",
    "            self.weight,\n",
    "            self.matrix_shape\n",
    "        )\n",
    "        \n",
    "        # Perform sparse matrix multiplication\n",
    "        return torch.sparse.mm(x, weight_matrix.t())\n",
    "    \n",
    "    @property\n",
    "    def sparsity(self) -> float:\n",
    "        \"\"\"Calculate the sparsity of the network (percentage of pruned weights).\"\"\"\n",
    "        return float(self._sparsity)\n",
    "    \n",
    "    def get_dense_weights(self) -> torch.Tensor:\n",
    "        \"\"\"Return the weight matrix in dense format (for analysis/comparison).\"\"\"\n",
    "        dense = torch.zeros(self.matrix_shape, device=self.weight.device)\n",
    "        dense[self.indices[0], self.indices[1]] = self.weight\n",
    "        return dense\n",
    "\n",
    "def create_pruned_model(nr: int, nc: int, M: np.ndarray) -> PrunedStateModelNet:\n",
    "    \"\"\"\n",
    "    Create an efficiently sparse model that only stores and computes with non-zero weights.\n",
    "    \n",
    "    Args:\n",
    "        nr: Number of rows in grid\n",
    "        nc: Number of columns in grid\n",
    "        M: The true model matrix to base sparsity pattern on\n",
    "        \n",
    "    Returns:\n",
    "        Initialized PrunedStateModelNet\n",
    "    \"\"\"\n",
    "    M_tensor = torch.from_numpy(M).float()\n",
    "    model = PrunedStateModelNet(nr, nc, M)\n",
    "    \n",
    "    # Print sparsity information\n",
    "    print(f\"Created pruned model with {model.sparsity*100:.2f}% sparsity\")\n",
    "    print(f\"Original parameters: {nr * nc * nr * nc}\")\n",
    "    print(f\"Non-zero parameters: {torch.sum(model.weight != 0).item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_dataset(\n",
    "    filename: str,\n",
    "    num_train_runs: int,\n",
    "    num_test_runs: int,\n",
    "    timesteps_per_run: int,\n",
    "    nr: int,\n",
    "    nc: int,\n",
    "    dt: float,\n",
    "    F: float\n",
    ") -> None:\n",
    "    # Generate a single set of parameters first\n",
    "    temp_dataset = StateModelDataset(1, 1, nr, nc, dt, F)\n",
    "    shared_params = temp_dataset.fixed_params\n",
    "    \n",
    "    # Generate the model matrix M using the helper function\n",
    "    M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "        nr=nr,\n",
    "        nc=nc,\n",
    "        dt=dt,\n",
    "        KX=shared_params['KX'].numpy(),\n",
    "        KY=shared_params['KY'].numpy(),\n",
    "        DX_C=shared_params['DX_C'].numpy(),\n",
    "        DY_C=shared_params['DY_C'].numpy(),\n",
    "        DX_G=shared_params['DX_G'].numpy(),\n",
    "        DY_G=shared_params['DY_G'].numpy(),\n",
    "        VX=shared_params['VX'].numpy(),\n",
    "        VY=shared_params['VY'].numpy(),\n",
    "        RAC=shared_params['RAC'].numpy(),\n",
    "        F=F,\n",
    "        cyclic_east_west=True,\n",
    "        cyclic_north_south=False,\n",
    "        M_is_sparse=False\n",
    "    )\n",
    "    \n",
    "    # Create dataset instances with shared parameters\n",
    "    train_dataset = StateModelDataset(num_train_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    test_dataset = StateModelDataset(num_test_runs, timesteps_per_run, nr, nc, dt, F, fixed_params=shared_params)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Save top-level data that's shared between train and test\n",
    "        f.create_dataset('model_matrix', data=M)  # Store M at top level\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['nr'] = nr\n",
    "        f.attrs['nc'] = nc\n",
    "        f.attrs['dt'] = dt\n",
    "        f.attrs['F'] = F\n",
    "        f.attrs['timesteps_per_run'] = timesteps_per_run\n",
    "        \n",
    "        # Save the shared parameters\n",
    "        params_group = f.create_group('shared_params')\n",
    "        for key, value in shared_params.items():\n",
    "            params_group.create_dataset(key, data=value.numpy())\n",
    "        \n",
    "        # Create train and test groups for the actual datasets\n",
    "        train_group = f.create_group('train')\n",
    "        test_group = f.create_group('test')\n",
    "        \n",
    "        # Helper function to save a single dataset\n",
    "        def save_dataset(group, dataset, desc):\n",
    "            samples_group = group.create_group('samples')\n",
    "            for i, (x_t, x_t_plus_1) in enumerate(tqdm(dataset, desc=desc)):\n",
    "                sample_group = samples_group.create_group(f'sample_{i}')\n",
    "                sample_group.create_dataset('x_t', data=x_t.numpy())\n",
    "                sample_group.create_dataset('x_t_plus_1', data=x_t_plus_1.numpy())\n",
    "        \n",
    "        # Save training and testing datasets\n",
    "        save_dataset(train_group, train_dataset, \"Saving training data\")\n",
    "        save_dataset(test_group, test_dataset, \"Saving testing data\")\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple[Dict, Dict]:\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Load top-level shared data\n",
    "        metadata = {\n",
    "            'nr': f.attrs['nr'],\n",
    "            'nc': f.attrs['nc'],\n",
    "            'dt': f.attrs['dt'],\n",
    "            'F': f.attrs['F'],\n",
    "            'timesteps_per_run': f.attrs['timesteps_per_run']\n",
    "        }\n",
    "        \n",
    "        # Load the model matrix M from top level\n",
    "        M = f['model_matrix'][:]\n",
    "        \n",
    "        # Load shared parameters\n",
    "        shared_params = {}\n",
    "        params_group = f['shared_params']\n",
    "        for key in params_group.keys():\n",
    "            shared_params[key] = torch.from_numpy(params_group[key][:]).float()\n",
    "        \n",
    "        def load_dataset(group):\n",
    "            data = []\n",
    "            samples_group = group['samples']\n",
    "            for sample_name in tqdm(samples_group.keys(), desc=f\"Loading {group.name} data\"):\n",
    "                sample = samples_group[sample_name]\n",
    "                x_t = torch.from_numpy(sample['x_t'][:]).float()\n",
    "                x_t_plus_1 = torch.from_numpy(sample['x_t_plus_1'][:]).float()\n",
    "                data.append((x_t, x_t_plus_1))\n",
    "            return data\n",
    "        \n",
    "        train_data = load_dataset(f['train'])\n",
    "        test_data = load_dataset(f['test'])\n",
    "    \n",
    "    # Create common shared data dictionary\n",
    "    shared_data = {\n",
    "        'metadata': metadata,\n",
    "        'shared_params': shared_params,\n",
    "        'model_matrix': M\n",
    "    }\n",
    "    \n",
    "    # Return dictionaries that combine shared data with specific train/test data\n",
    "    return {\n",
    "        'train': train_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }, {\n",
    "        'test': test_data,\n",
    "        **shared_data  # Unpack shared data\n",
    "    }\n",
    "\n",
    "\n",
    "class SavedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pre-saved data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def create_dataloaders(train_data, test_data, batch_size=32, shuffle_train=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader objects for training and testing datasets.\n",
    "    Now handles data without parameters.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training dataset dictionary containing 'train' data\n",
    "        test_data: Testing dataset dictionary containing 'test' data\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        shuffle_train: Whether to shuffle training data\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoader objects\n",
    "    \"\"\"\n",
    "    train_dataset = SavedDataset(train_data['train'])\n",
    "    test_dataset = SavedDataset(test_data['test'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset with 180000 training samples and 18000 testing samples\n",
      "Generating world 1/1\n",
      "Generating world 1/20000\n",
      "Generating world 101/20000\n",
      "Generating world 201/20000\n",
      "Generating world 301/20000\n",
      "Generating world 401/20000\n",
      "Generating world 501/20000\n",
      "Generating world 601/20000\n",
      "Generating world 701/20000\n",
      "Generating world 801/20000\n",
      "Generating world 901/20000\n",
      "Generating world 1001/20000\n",
      "Generating world 1101/20000\n",
      "Generating world 1201/20000\n",
      "Generating world 1301/20000\n",
      "Generating world 1401/20000\n",
      "Generating world 1501/20000\n",
      "Generating world 1601/20000\n",
      "Generating world 1701/20000\n",
      "Generating world 1801/20000\n",
      "Generating world 1901/20000\n",
      "Generating world 2001/20000\n",
      "Generating world 2101/20000\n",
      "Generating world 2201/20000\n",
      "Generating world 2301/20000\n",
      "Generating world 2401/20000\n",
      "Generating world 2501/20000\n",
      "Generating world 2601/20000\n",
      "Generating world 2701/20000\n",
      "Generating world 2801/20000\n",
      "Generating world 2901/20000\n",
      "Generating world 3001/20000\n",
      "Generating world 3101/20000\n",
      "Generating world 3201/20000\n",
      "Generating world 3301/20000\n",
      "Generating world 3401/20000\n",
      "Generating world 3501/20000\n",
      "Generating world 3601/20000\n",
      "Generating world 3701/20000\n",
      "Generating world 3801/20000\n",
      "Generating world 3901/20000\n",
      "Generating world 4001/20000\n",
      "Generating world 4101/20000\n",
      "Generating world 4201/20000\n",
      "Generating world 4301/20000\n",
      "Generating world 4401/20000\n",
      "Generating world 4501/20000\n",
      "Generating world 4601/20000\n",
      "Generating world 4701/20000\n",
      "Generating world 4801/20000\n",
      "Generating world 4901/20000\n",
      "Generating world 5001/20000\n",
      "Generating world 5101/20000\n",
      "Generating world 5201/20000\n",
      "Generating world 5301/20000\n",
      "Generating world 5401/20000\n",
      "Generating world 5501/20000\n",
      "Generating world 5601/20000\n",
      "Generating world 5701/20000\n",
      "Generating world 5801/20000\n",
      "Generating world 5901/20000\n",
      "Generating world 6001/20000\n",
      "Generating world 6101/20000\n",
      "Generating world 6201/20000\n",
      "Generating world 6301/20000\n",
      "Generating world 6401/20000\n",
      "Generating world 6501/20000\n",
      "Generating world 6601/20000\n",
      "Generating world 6701/20000\n",
      "Generating world 6801/20000\n",
      "Generating world 6901/20000\n",
      "Generating world 7001/20000\n",
      "Generating world 7101/20000\n",
      "Generating world 7201/20000\n",
      "Generating world 7301/20000\n",
      "Generating world 7401/20000\n",
      "Generating world 7501/20000\n",
      "Generating world 7601/20000\n",
      "Generating world 7701/20000\n",
      "Generating world 7801/20000\n",
      "Generating world 7901/20000\n",
      "Generating world 8001/20000\n",
      "Generating world 8101/20000\n",
      "Generating world 8201/20000\n",
      "Generating world 8301/20000\n",
      "Generating world 8401/20000\n",
      "Generating world 8501/20000\n",
      "Generating world 8601/20000\n",
      "Generating world 8701/20000\n",
      "Generating world 8801/20000\n",
      "Generating world 8901/20000\n",
      "Generating world 9001/20000\n",
      "Generating world 9101/20000\n",
      "Generating world 9201/20000\n",
      "Generating world 9301/20000\n",
      "Generating world 9401/20000\n",
      "Generating world 9501/20000\n",
      "Generating world 9601/20000\n",
      "Generating world 9701/20000\n",
      "Generating world 9801/20000\n",
      "Generating world 9901/20000\n",
      "Generating world 10001/20000\n",
      "Generating world 10101/20000\n",
      "Generating world 10201/20000\n",
      "Generating world 10301/20000\n",
      "Generating world 10401/20000\n",
      "Generating world 10501/20000\n",
      "Generating world 10601/20000\n",
      "Generating world 10701/20000\n",
      "Generating world 10801/20000\n",
      "Generating world 10901/20000\n",
      "Generating world 11001/20000\n",
      "Generating world 11101/20000\n",
      "Generating world 11201/20000\n",
      "Generating world 11301/20000\n",
      "Generating world 11401/20000\n",
      "Generating world 11501/20000\n",
      "Generating world 11601/20000\n",
      "Generating world 11701/20000\n",
      "Generating world 11801/20000\n",
      "Generating world 11901/20000\n",
      "Generating world 12001/20000\n",
      "Generating world 12101/20000\n",
      "Generating world 12201/20000\n",
      "Generating world 12301/20000\n",
      "Generating world 12401/20000\n",
      "Generating world 12501/20000\n",
      "Generating world 12601/20000\n",
      "Generating world 12701/20000\n",
      "Generating world 12801/20000\n",
      "Generating world 12901/20000\n",
      "Generating world 13001/20000\n",
      "Generating world 13101/20000\n",
      "Generating world 13201/20000\n",
      "Generating world 13301/20000\n",
      "Generating world 13401/20000\n",
      "Generating world 13501/20000\n",
      "Generating world 13601/20000\n",
      "Generating world 13701/20000\n",
      "Generating world 13801/20000\n",
      "Generating world 13901/20000\n",
      "Generating world 14001/20000\n",
      "Generating world 14101/20000\n",
      "Generating world 14201/20000\n",
      "Generating world 14301/20000\n",
      "Generating world 14401/20000\n",
      "Generating world 14501/20000\n",
      "Generating world 14601/20000\n",
      "Generating world 14701/20000\n",
      "Generating world 14801/20000\n",
      "Generating world 14901/20000\n",
      "Generating world 15001/20000\n",
      "Generating world 15101/20000\n",
      "Generating world 15201/20000\n",
      "Generating world 15301/20000\n",
      "Generating world 15401/20000\n",
      "Generating world 15501/20000\n",
      "Generating world 15601/20000\n",
      "Generating world 15701/20000\n",
      "Generating world 15801/20000\n",
      "Generating world 15901/20000\n",
      "Generating world 16001/20000\n",
      "Generating world 16101/20000\n",
      "Generating world 16201/20000\n",
      "Generating world 16301/20000\n",
      "Generating world 16401/20000\n",
      "Generating world 16501/20000\n",
      "Generating world 16601/20000\n",
      "Generating world 16701/20000\n",
      "Generating world 16801/20000\n",
      "Generating world 16901/20000\n",
      "Generating world 17001/20000\n",
      "Generating world 17101/20000\n",
      "Generating world 17201/20000\n",
      "Generating world 17301/20000\n",
      "Generating world 17401/20000\n",
      "Generating world 17501/20000\n",
      "Generating world 17601/20000\n",
      "Generating world 17701/20000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate and save datasets\n",
    "nr,nc = 64,64\n",
    "num_train_runs = 20000\n",
    "num_test_runs = 2000\n",
    "timesteps_per_run = 10\n",
    "num_train_data = num_train_runs * (timesteps_per_run - 1)\n",
    "num_test_data = num_test_runs * (timesteps_per_run - 1)\n",
    "print(f\"Generating dataset with {num_train_data} training samples and {num_test_data} testing samples\")\n",
    "filename = f'data_forward-sim_cov_{nr}x{nc}_train{num_train_data}_test{num_test_data}.h5'\n",
    "#filename = f'data_forward-sim_{nr}x{nc}_train{num_train_data}_test{num_test_data}.h5'\n",
    "\n",
    "generate_and_save_dataset(\n",
    "    filename=filename,\n",
    "    num_train_runs=num_train_runs,      # Number of different runs for training\n",
    "    num_test_runs=num_test_runs,        # Number of different runs for testing\n",
    "    timesteps_per_run=10,      # Number of timesteps per run\n",
    "    nr=nr,\n",
    "    nc=nc,\n",
    "    dt=0.01,\n",
    "    F=0.01\n",
    ")\n",
    "\"\"\"\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# Load dataset and create dataloaders\n",
    "train_data, test_data = load_dataset('state_model_data_one-world.h5')\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    train_data, \n",
    "    test_data, \n",
    "    batch_size=32\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: str = 'cpu',\n",
    "    run_number: int = None\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Get total dataset sizes\n",
    "    train_size = len(train_dataloader.dataset)\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "    batch_size_train = train_dataloader.batch_size\n",
    "    batch_size_test = test_dataloader.batch_size\n",
    "    \n",
    "    run_str = f\" (Run {run_number})\" if run_number is not None else \"\"\n",
    "    print(f\"Training started{run_str}...\")\n",
    "    \n",
    "    # Calculate initial losses before any training\n",
    "    model.eval()\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initial training loss\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        # Initial test loss\n",
    "        for x_t, x_t_plus_1 in test_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            total_test_loss += loss.item()\n",
    "    \n",
    "    # Add initial losses (pre-training)\n",
    "    initial_train_loss = total_train_loss * batch_size_train / train_size\n",
    "    initial_test_loss = total_test_loss * batch_size_test / test_size\n",
    "    train_losses.append(initial_train_loss)\n",
    "    test_losses.append(initial_test_loss)\n",
    "    \n",
    "    print(f\"Initial training loss: {initial_train_loss:.6f}\")\n",
    "    print(f\"Initial test loss: {initial_test_loss:.6f}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_t, x_t_plus_1 in train_dataloader:\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_t_plus_1 in test_dataloader:\n",
    "                x_t = x_t.to(device)\n",
    "                x_t_plus_1 = x_t_plus_1.to(device)\n",
    "                \n",
    "                predicted = model(x_t)\n",
    "                test_loss = criterion(predicted, x_t_plus_1)\n",
    "                total_test_loss += test_loss.item()\n",
    "        \n",
    "        # Normalize losses by total dataset size instead of number of batches\n",
    "        avg_train_loss = total_train_loss * batch_size_train / train_size\n",
    "        avg_test_loss = total_test_loss * batch_size_test / test_size\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.6f}\")\n",
    "            print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Training complete{run_str}!\")\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "def simulate_state_evolution(model, M, x0, num_timesteps, F, f, device='cpu'):\n",
    "    \"\"\"\n",
    "    Simulates state evolution using both the trained model and true model M.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained neural network\n",
    "        M: True model matrix\n",
    "        x0: Initial state\n",
    "        num_timesteps: Number of timesteps to simulate\n",
    "        F: Forcing parameter\n",
    "        f: Forcing field\n",
    "        device: Device to run model on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (neural_net_states, true_model_states)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x0_tensor = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "    f_tensor = torch.tensor(f, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Lists to store states over time\n",
    "    neural_net_states = [x0]\n",
    "    true_model_states = [x0]\n",
    "    \n",
    "    # Simulate using neural network\n",
    "    with torch.no_grad():\n",
    "        current_state = x0_tensor\n",
    "        for t in range(num_timesteps):\n",
    "            next_state = model(current_state.unsqueeze(0)).squeeze(0)\n",
    "            neural_net_states.append(next_state.cpu().numpy())\n",
    "            current_state = next_state\n",
    "    \n",
    "    # Simulate using true model\n",
    "    current_state = x0\n",
    "    for t in range(num_timesteps):\n",
    "        next_state = M @ current_state + F * f\n",
    "        true_model_states.append(next_state)\n",
    "        current_state = next_state\n",
    "    \n",
    "    return neural_net_states, true_model_states\n",
    "\n",
    "def train_multiple_models(\n",
    "    nr: int, \n",
    "    nc: int, \n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    train_data: dict,\n",
    "    num_runs: int = 10,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    device: str = 'cpu',\n",
    "    num_timesteps_sim: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Train multiple models and compare their simulation results with the true model.\n",
    "    Displays 5 evenly spaced timesteps in the visualization.\n",
    "    \n",
    "    Args:\n",
    "        nr: Number of rows in the grid\n",
    "        nc: Number of columns in the grid\n",
    "        train_dataloader: DataLoader for training data\n",
    "        test_dataloader: DataLoader for test data\n",
    "        train_data: Dictionary containing model matrix, metadata, and shared parameters\n",
    "        num_runs: Number of models to train\n",
    "        num_epochs: Number of epochs per training run\n",
    "        learning_rate: Learning rate for optimization\n",
    "        device: Device to run training on\n",
    "        num_timesteps_sim: Number of timesteps to simulate\n",
    "    \"\"\"\n",
    "    all_models = []\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    \n",
    "    # Extract simulation parameters from train_data\n",
    "    M = train_data['model_matrix']\n",
    "    metadata = train_data['metadata']\n",
    "    f = train_data['shared_params']['f'].numpy()\n",
    "    \n",
    "    # Calculate the indices for 5 evenly spaced timesteps\n",
    "    display_timesteps = [\n",
    "        0,  # First timestep\n",
    "        num_timesteps_sim // 4,  # Quarter way\n",
    "        num_timesteps_sim // 2,  # Halfway\n",
    "        3 * num_timesteps_sim // 4,  # Three-quarters way\n",
    "        num_timesteps_sim  # Last timestep\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\nStarting model {i+1}/{num_runs}\")\n",
    "        model = PrunedStateModelNet(nr, nc, M)\n",
    "        \n",
    "        trained_model, train_losses, test_losses = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            device=device,\n",
    "            run_number=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot individual run results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(0, num_epochs + 1)  # Start from 0 to include initial loss\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "        plt.title(f'Training and Testing Loss Over Time - Run {i+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # After training, compare model simulations\n",
    "        # Get sample initial state from test data\n",
    "        sample_batch = next(iter(test_dataloader))\n",
    "        x0 = sample_batch[0][0].numpy()  # Take first state from first batch\n",
    "        \n",
    "        # Simulate evolution using both models\n",
    "        neural_net_states, true_model_states = simulate_state_evolution(\n",
    "            trained_model, M, x0, num_timesteps=num_timesteps_sim, F=metadata['F'], f=f, device=device\n",
    "        )\n",
    "        \n",
    "        # Extract only the desired timesteps for visualization\n",
    "        neural_net_display = [neural_net_states[t] for t in display_timesteps]\n",
    "        true_model_display = [true_model_states[t] for t in display_timesteps]\n",
    "        \n",
    "        # Plot comparison\n",
    "        helper.plot_multi_heatmap_time_evolution(\n",
    "            saved_timesteps=display_timesteps,  # Use the display timesteps\n",
    "            many_states_over_time=[neural_net_display, true_model_display],\n",
    "            nr=nr,\n",
    "            nc=nc,\n",
    "            titles=[\"Neural Network\", \"True Model\"],\n",
    "            big_title=f\"State Evolution Comparison - Model {i+1}\\nTimesteps {display_timesteps}\",\n",
    "            vmin=None,\n",
    "            vmax=None\n",
    "        )\n",
    "        \n",
    "        all_models.append(trained_model)\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_test_losses.append(test_losses)\n",
    "    \n",
    "    return all_models, all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = load_dataset(filename)\n",
    "train_loader = DataLoader(SavedDataset(train_data['train']), batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(SavedDataset(test_data['test']), batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "M = train_data['model_matrix']\n",
    "metadata = train_data['metadata']\n",
    "nr, nc = metadata['nr'], metadata['nc']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train multiple models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "models, train_losses, test_losses = train_multiple_models(\n",
    "    nr=nr,\n",
    "    nc=nc,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    train_data=train_data,  # Pass in the train_data dictionary\n",
    "    num_runs=1,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    num_timesteps_sim=100\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How much smaller can we make it? - Vince\n",
    "\n",
    "Previous problems: \n",
    "* Accidentally created different model for testing and training\n",
    "* Parameters were too big, caused nasty divergence (heat temps exploding way too high)\n",
    "\n",
    "Comments:\n",
    "- Simplest linear model is perfect\n",
    "- Other models not so much\n",
    "- ReLU vs Linear hidden layer not super different\n",
    "- Smallest network is worse, as expected\n",
    "- No longer varies much between initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare learned network to M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def extract_linear_params(model: torch.nn.Module) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract the weight matrix A and bias vector b from a linear neural network.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model with a single linear layer\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - A: Weight matrix as numpy array\n",
    "        - b: Bias vector as numpy array\n",
    "    \"\"\"\n",
    "    # Get the first (and only) linear layer\n",
    "    linear_layer = model.fc1\n",
    "    \n",
    "    # Extract weights and biases\n",
    "    A = linear_layer.weight.detach().cpu().numpy()\n",
    "    b = linear_layer.bias.detach().cpu().numpy() if linear_layer.bias is not None else np.zeros(A.shape[0])\n",
    "    \n",
    "    return A, b\n",
    "\n",
    "def compare_matrices(A: np.ndarray, M: np.ndarray, b: np.ndarray, \n",
    "                    f: Optional[np.ndarray] = None, F: Optional[float] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Compare the learned matrix A with the true matrix M and analyze differences.\n",
    "    \n",
    "    Args:\n",
    "        A: Weight matrix from neural network\n",
    "        M: True model matrix\n",
    "        b: Bias vector from neural network\n",
    "        f: Optional forcing field\n",
    "        F: Optional forcing parameter\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing comparison metrics\n",
    "    \"\"\"\n",
    "    # Calculate basic matrix differences\n",
    "    diff = A - M\n",
    "    rel_diff = np.abs(diff) / (np.abs(M) + 1e-10)  # Add small epsilon to avoid division by zero\n",
    "    #Mask relative differences for values that are 0 in M: check M\n",
    "    rel_diff[np.abs(M) < 1e-10] = 0\n",
    "\n",
    "    # Create masks for off-diagonal elements\n",
    "    mask = ~np.eye(M.shape[0], dtype=bool)\n",
    "    \n",
    "    # Get off-diagonal elements only\n",
    "    diff_no_diag = diff[mask]\n",
    "    M_no_diag = M[mask]\n",
    "    A_no_diag = A[mask]\n",
    "    \n",
    "    # Compare bias with forcing term if provided\n",
    "    if f is not None and F is not None:\n",
    "        forcing_term = F * f\n",
    "        bias_diff = b - forcing_term\n",
    "        bias_rel_diff = np.abs(bias_diff) / (np.abs(forcing_term) + 1e-10)\n",
    "    else:\n",
    "        bias_diff = None\n",
    "        bias_rel_diff = None\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    metrics = {\n",
    "        'magn(M)/magn(A)': np.linalg.norm(M)/np.linalg.norm(A),\n",
    "        'magn(M_no_diag)/magn(A_no_diag)': np.linalg.norm(M_no_diag)/np.linalg.norm(A_no_diag),\n",
    "        'magn(diff)/magn(M)': np.linalg.norm(diff) / np.linalg.norm(M),\n",
    "        'magn(diff_no_diag)/magn(M_no_diag)': np.linalg.norm(diff_no_diag) / np.linalg.norm(M_no_diag),\n",
    "        'normalized dot product': np.dot(A.flatten(), M.flatten()) / (np.linalg.norm(A) * np.linalg.norm(M)),\n",
    "        'no-diag normalized dot product': np.dot(A_no_diag, M_no_diag) / (np.linalg.norm(A_no_diag) * np.linalg.norm(M_no_diag))\n",
    "    }\n",
    "    \n",
    "    if bias_diff is not None:\n",
    "        metrics.update({\n",
    "            'magnitude(bias_diff)/magnitude(forcing_term)': np.linalg.norm(bias_diff) / np.linalg.norm(forcing_term),\n",
    "            'normalized dot product (bias)': np.dot(b.flatten(), forcing_term.flatten()) / (np.linalg.norm(b) * np.linalg.norm(forcing_term))\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_comparison(A: np.ndarray, M: np.ndarray, b: np.ndarray, \n",
    "                        f: Optional[np.ndarray] = None, F: Optional[float] = None,\n",
    "                        nr: int = None, nc: int = None) -> None:\n",
    "    \"\"\"\n",
    "    Create visualizations comparing the matrices and bias terms.\n",
    "    \n",
    "    Args:\n",
    "        A: Weight matrix from neural network\n",
    "        M: True model matrix\n",
    "        b: Bias vector from neural network\n",
    "        f: Optional forcing field\n",
    "        F: Optional forcing parameter\n",
    "        nr: Number of rows in grid (for reshaping)\n",
    "        nc: Number of columns in grid (for reshaping)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # Create masked versions of matrices\n",
    "    A_masked = A.copy()\n",
    "    M_masked = M.copy()\n",
    "    mask = np.eye(M.shape[0], dtype=bool)\n",
    "    A_masked[mask] = 0  # or use 0 instead of np.nan if preferred\n",
    "    M_masked[mask] = 0  # or use 0 instead of np.nan if preferred\n",
    "\n",
    "\n",
    "    # Plot matrices\n",
    "    im1 = axes[0,0].imshow( A_masked)\n",
    "    axes[0,0].set_title('Neural Network Matrix (A) with diagonal masked')\n",
    "    plt.colorbar(im1, ax=axes[0,0])\n",
    "    \n",
    "    im2 = axes[0,1].imshow(M_masked)\n",
    "    axes[0,1].set_title('True Model Matrix (M) with diagonal masked')\n",
    "    plt.colorbar(im2, ax=axes[0,1])\n",
    "    \n",
    "    # Plot difference\n",
    "    diff = A_masked - M_masked #A - M\n",
    "    im3 = axes[1,0].imshow(diff)\n",
    "    axes[1,0].set_title('Difference (A - M)')\n",
    "    plt.colorbar(im3, ax=axes[1,0])\n",
    "    \n",
    "    # Plot bias comparison if forcing term available\n",
    "    if f is not None and F is not None:\n",
    "        forcing_term = F * f\n",
    "        if nr is not None and nc is not None:\n",
    "            # Reshape for 2D visualization if dimensions provided\n",
    "            b_reshaped = b.reshape(nr, nc)\n",
    "            forcing_reshaped = forcing_term.reshape(nr, nc)\n",
    "            bias_diff = b_reshaped - forcing_reshaped\n",
    "        else:\n",
    "            bias_diff = b - forcing_term\n",
    "            \n",
    "        im4 = axes[1,1].imshow(bias_diff)\n",
    "        axes[1,1].set_title('Bias - Forcing Term Difference')\n",
    "        plt.colorbar(im4, ax=axes[1,1])\n",
    "    else:\n",
    "        if nr is not None and nc is not None:\n",
    "            b_reshaped = b.reshape(nr, nc)\n",
    "        else:\n",
    "            b_reshaped = b\n",
    "        im4 = axes[1,1].imshow(b_reshaped)\n",
    "        axes[1,1].set_title('Bias Term')\n",
    "        plt.colorbar(im4, ax=axes[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_model(model: torch.nn.Module, M: np.ndarray, f: Optional[np.ndarray] = None, \n",
    "                 F: Optional[float] = None, nr: Optional[int] = None, nc: Optional[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Complete analysis of a trained model compared to true system.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        M: True model matrix\n",
    "        f: Optional forcing field\n",
    "        F: Optional forcing parameter\n",
    "        nr: Optional number of rows for reshaping\n",
    "        nc: Optional number of columns for reshaping\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    A, b = extract_linear_params(model)\n",
    "    \n",
    "    # Compare matrices\n",
    "    metrics = compare_matrices(A, M, b, f, F)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"Comparison Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.6e}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    visualize_comparison(A, M, b, f, F, nr, nc)\n",
    "    \n",
    "    return A, b, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the trained model from the final cell\n",
    "if len(models) > 0:\n",
    "    trained_model = models[0]  # Get first model since num_runs=1\n",
    "    \n",
    "    A, b, metrics = analyze_model(\n",
    "        model=trained_model,\n",
    "        M=M,  # The true model matrix \n",
    "        f=train_data['shared_params']['f'].numpy(),  # The forcing field\n",
    "        F=train_data['metadata']['F'],  # The forcing parameter\n",
    "        nr=train_data['metadata']['nr'],  # Number of rows\n",
    "        nc=train_data['metadata']['nc']  # Number of columns\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No trained models found in the 'models' container.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = trained_model\n",
    "\n",
    "# Use the first input-output pair from the test data: not the whole batch, just the first data point\n",
    "sample_batch = next(iter(test_loader))\n",
    "input_batch, output_batch = sample_batch\n",
    "\n",
    "input_data = input_batch[0].unsqueeze(0)\n",
    "output_data = output_batch[0].unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_gradients(model, input_data, output_data, M, nr=None, nc=None):\n",
    "    \"\"\"\n",
    "    Analyze the gradient matrix of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        input_data: Input tensor\n",
    "        output_data: Target output tensor\n",
    "        nr: Number of rows (for reshaping visualization)\n",
    "        nc: Number of columns (for reshaping visualization)\n",
    "    \"\"\"\n",
    "    # Compute Jacobian\n",
    "    jacobian = torch.autograd.functional.jacobian(model, input_data)\n",
    "    jacobian = jacobian.view(output_data.size(-1), input_data.size(-1))\n",
    "    jacobian = jacobian.detach().numpy()\n",
    "    \n",
    "    # For a linear model, extract the weight matrix for comparison\n",
    "    if not hasattr(model, 'fc2'):\n",
    "        \n",
    "        # Compute difference between Jacobian and weight matrix\n",
    "        diff = jacobian - M\n",
    "        max_diff = np.max(np.abs(diff))\n",
    "        print(f\"Maximum difference between Jacobian and weight matrix: {max_diff}\")\n",
    "        \n",
    "        if max_diff < 1e-6:\n",
    "            print(\"Gradient matrix is effectively identical to weight matrix (as expected for a linear model)\")\n",
    "        else:\n",
    "            print(\"Warning: Gradient matrix differs from weight matrix - model may not be purely linear\")\n",
    "    \n",
    "    # Visualize the gradient matrix\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(jacobian)\n",
    "    plt.colorbar()\n",
    "    plt.title('Gradient Matrix (Jacobian)')\n",
    "    \n",
    "    # If spatial dimensions are provided, show example input and output\n",
    "    if nr is not None and nc is not None:\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(input_data.detach().numpy().reshape(nr, nc))\n",
    "        plt.colorbar()\n",
    "        plt.title('Input State')\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.imshow(output_data.detach().numpy().reshape(nr, nc))\n",
    "        plt.colorbar()\n",
    "        plt.title('Target Output State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return jacobian\n",
    "\n",
    "# Example usage:\n",
    "jacobian = analyze_gradients(model, input_data, output_data, M, nr=64, nc=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
