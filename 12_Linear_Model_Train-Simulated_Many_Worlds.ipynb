{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 1: Valid Output+Random Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import helper\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class LinearModelDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, nr: int, nc: int, dt: float, F: float):\n",
    "        self.num_samples = num_samples\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        self.dt = dt\n",
    "        self.F = F\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def _generate_gaussian_field(self, n, nrv, ncv):\n",
    "        \"\"\"\n",
    "        Generate a Gaussian field composed of n Gaussian functions.\n",
    "        \"\"\"\n",
    "        mux = np.random.choice(ncv, n)\n",
    "        muy = np.random.choice(range(2, nrv - 2), n)\n",
    "        sigmax = np.random.uniform(1, ncv/4, n)\n",
    "        sigmay = np.random.uniform(1, nrv/4, n)\n",
    "\n",
    "        v = np.zeros((nrv, ncv))\n",
    "        for i in range(n):\n",
    "            for x in range(ncv):\n",
    "                for y in range(nrv):\n",
    "                    # Create three copies for pseudo-periodic field\n",
    "                    gauss = np.exp(-((x-mux[i])**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]-ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]+ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    v[y,x] += gauss\n",
    "        return v\n",
    "\n",
    "    def _generate_circular_field(self, v):\n",
    "        \"\"\"\n",
    "        Generate a circular field from gradient of input field.\n",
    "        \"\"\"\n",
    "        grad_v_y, grad_v_x = np.gradient(v)\n",
    "        return -grad_v_y, grad_v_x\n",
    "\n",
    "    def _generate_params(self):\n",
    "        \"\"\"\n",
    "        Generate model parameters similar to generate_world function.\n",
    "        \"\"\"\n",
    "        # Base grid parameters\n",
    "        DX_C = torch.ones(self.nr, self.nc + 1)\n",
    "        DY_C = torch.ones(self.nr + 1, self.nc)\n",
    "        DX_G = torch.ones(self.nr + 1, self.nc)\n",
    "        DY_G = torch.ones(self.nr, self.nc + 1)\n",
    "        RAC = torch.ones(self.nr, self.nc)\n",
    "\n",
    "        # Generate random diffusivities (must be positive)\n",
    "        KX = torch.abs(torch.rand(self.nr, self.nc + 1))\n",
    "        KY = torch.abs(torch.rand(self.nr + 1, self.nc))\n",
    "\n",
    "        # Generate velocities using Gaussian field approach\n",
    "        num_gauss = 16  # Number of Gaussian functions for velocity field\n",
    "        gauss = self._generate_gaussian_field(num_gauss, self.nr + 1, self.nc + 1)\n",
    "        VX_np, VY_np = self._generate_circular_field(gauss)\n",
    "        \n",
    "        # Convert velocities to PyTorch and scale\n",
    "        VX = torch.from_numpy(100 * VX_np[:-1, :]).float()\n",
    "        VY = torch.from_numpy(100 * VY_np[:, :-1]).float()\n",
    "\n",
    "        # Create random forcing term\n",
    "        f = torch.randn(self.nr * self.nc)\n",
    "\n",
    "        return {\n",
    "            'KX': KX,\n",
    "            'KY': KY,\n",
    "            'DX_C': DX_C,\n",
    "            'DY_C': DY_C,\n",
    "            'DX_G': DX_G,\n",
    "            'DY_G': DY_G,\n",
    "            'VX': VX,\n",
    "            'VY': VY,\n",
    "            'RAC': RAC,\n",
    "            'f': f,\n",
    "        }\n",
    "\n",
    "    def _generate_samples(self) -> list:\n",
    "        \"\"\"\n",
    "        Generate multiple samples with parameters.\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        for _ in range(self.num_samples):\n",
    "            params = self._generate_params()\n",
    "            \n",
    "            x_t = torch.randn(self.nr * self.nc)\n",
    "            lambda_t_plus_1 = torch.randn(self.nr * self.nc)\n",
    "            dx_dt_lambda = self._compute_dx_dt_lambda(params, x_t, lambda_t_plus_1)\n",
    "            \n",
    "            samples.append((params, x_t, lambda_t_plus_1, dx_dt_lambda))\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def _compute_dx_dt_lambda(self, params, x_t, lambda_t_plus_1):\n",
    "        \"\"\"\n",
    "        Compute dx(t+1)/dx(t) * lambda(t+1) for the linear model.\n",
    "        \"\"\"\n",
    "        # Convert PyTorch tensors to numpy arrays for helper function\n",
    "        np_params = {\n",
    "            key: tensor.cpu().detach().numpy() if torch.is_tensor(tensor) else tensor\n",
    "            for key, tensor in params.items()\n",
    "        }\n",
    "        \n",
    "        # Get model matrix M using helper function\n",
    "        M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "            nr=self.nr,\n",
    "            nc=self.nc,\n",
    "            dt=self.dt,\n",
    "            KX=np_params['KX'],\n",
    "            KY=np_params['KY'],\n",
    "            DX_C=np_params['DX_C'],\n",
    "            DY_C=np_params['DY_C'], \n",
    "            DX_G=np_params['DX_G'],\n",
    "            DY_G=np_params['DY_G'],\n",
    "            VX=np_params['VX'],\n",
    "            VY=np_params['VY'],\n",
    "            RAC=np_params['RAC'],\n",
    "            F=self.F,\n",
    "            cyclic_east_west=True,\n",
    "            cyclic_north_south=False,\n",
    "            M_is_sparse=False\n",
    "        )\n",
    "        \n",
    "        # Convert to numpy arrays and do matrix multiplication\n",
    "        lambda_np = lambda_t_plus_1.cpu().detach().numpy()\n",
    "        result_np = M.T @ lambda_np\n",
    "        \n",
    "        # Convert back to PyTorch tensor\n",
    "        return torch.from_numpy(result_np).float()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.samples[idx]\n",
    "\n",
    "class LinearModelNet(torch.nn.Module):\n",
    "    def __init__(self, nr: int, nc: int):\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.param_size = (nr * (nc + 1) +\n",
    "                           (nr + 1) * nc +\n",
    "                           nr * (nc + 1) +\n",
    "                           (nr + 1) * nc +\n",
    "                           (nr + 1) * nc +\n",
    "                           nr * (nc + 1) +\n",
    "                           nr * (nc + 1) +\n",
    "                           (nr + 1) * nc +\n",
    "                           nr * nc +\n",
    "                           nr * nc)\n",
    "        \n",
    "        self.input_size = self.state_size * 2 + self.param_size\n",
    "        self.output_size = nr * nc\n",
    "        \n",
    "        print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        # Adjust the architecture to handle the correct input size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 2048)\n",
    "        self.fc2 = torch.nn.Linear(2048, 1024)\n",
    "        self.fc3 = torch.nn.Linear(1024, 512)\n",
    "        self.fc4 = torch.nn.Linear(512, self.output_size)\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, lambda_t_plus_1: torch.Tensor, params: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        batch_size = x.size(0) \n",
    "        \n",
    "        # Ensure all tensors have the correct batch size\n",
    "        x = x.view(batch_size, -1)\n",
    "        lambda_t_plus_1 = lambda_t_plus_1.view(batch_size, -1)\n",
    "        \n",
    "        # Flatten the params dictionary\n",
    "        params_flat = torch.cat([param.view(batch_size, -1) for param in params.values()], dim=1)\n",
    "        \n",
    "        # Concatenate all inputs\n",
    "        x = torch.cat([x, lambda_t_plus_1, params_flat], dim=1)\n",
    "        \n",
    "        # Add some debugging print statements\n",
    "        #print(f\"x shape: {x.shape}\")\n",
    "        #print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_model(model: torch.nn.Module, dataloader: DataLoader, num_epochs: int, learning_rate: float):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for params, x_t, lambda_t_plus_1, dx_dt_lambda in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_t, lambda_t_plus_1, params)\n",
    "            loss = criterion(output, dx_dt_lambda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving training data: 100%|██████████| 180000/180000 [06:42<00:00, 446.88it/s]\n",
      "Saving testing data: 100%|██████████| 20000/20000 [00:43<00:00, 454.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_and_save_dataset(\n",
    "    filename: str,\n",
    "    num_train_samples: int,\n",
    "    num_test_samples: int,\n",
    "    nr: int,\n",
    "    nc: int,\n",
    "    dt: float,\n",
    "    F: float\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate training and testing datasets and save them to an HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the file to save the data to\n",
    "        num_train_samples: Number of training samples to generate\n",
    "        num_test_samples: Number of testing samples to generate\n",
    "        nr, nc: Grid dimensions\n",
    "        dt: Time step\n",
    "        F: Forcing parameter\n",
    "    \"\"\"\n",
    "    # Create dataset instances\n",
    "    train_dataset = LinearModelDataset(num_train_samples, nr, nc, dt, F)\n",
    "    test_dataset = LinearModelDataset(num_test_samples, nr, nc, dt, F)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Create train and test groups\n",
    "        train_group = f.create_group('train')\n",
    "        test_group = f.create_group('test')\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['nr'] = nr\n",
    "        f.attrs['nc'] = nc\n",
    "        f.attrs['dt'] = dt\n",
    "        f.attrs['F'] = F\n",
    "        \n",
    "        # Helper function to save a single dataset\n",
    "        def save_dataset(group, dataset, desc):\n",
    "            for i, (params, x_t, lambda_t_plus_1, dx_dt_lambda) in enumerate(tqdm(dataset, desc=desc)):\n",
    "                sample_group = group.create_group(f'sample_{i}')\n",
    "                \n",
    "                # Save parameters\n",
    "                params_group = sample_group.create_group('params')\n",
    "                for key, value in params.items():\n",
    "                    params_group.create_dataset(key, data=value.numpy())\n",
    "                \n",
    "                # Save state and outputs\n",
    "                sample_group.create_dataset('x_t', data=x_t.numpy())\n",
    "                sample_group.create_dataset('lambda_t_plus_1', data=lambda_t_plus_1.numpy())\n",
    "                sample_group.create_dataset('dx_dt_lambda', data=dx_dt_lambda.numpy())\n",
    "        \n",
    "        # Save training and testing datasets\n",
    "        save_dataset(train_group, train_dataset, \"Saving training data\")\n",
    "        save_dataset(test_group, test_dataset, \"Saving testing data\")\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Load training and testing datasets from an HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the file to load the data from\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of dictionaries containing training and testing data\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Load metadata\n",
    "        metadata = {\n",
    "            'nr': f.attrs['nr'],\n",
    "            'nc': f.attrs['nc'],\n",
    "            'dt': f.attrs['dt'],\n",
    "            'F': f.attrs['F']\n",
    "        }\n",
    "        \n",
    "        # Helper function to load a single dataset\n",
    "        def load_dataset(group):\n",
    "            data = []\n",
    "            for sample_name in tqdm(group.keys(), desc=f\"Loading {group.name} data\"):\n",
    "                sample = group[sample_name]\n",
    "                \n",
    "                # Load parameters\n",
    "                params = {\n",
    "                    key: torch.from_numpy(value[:]).float()\n",
    "                    for key, value in sample['params'].items()\n",
    "                }\n",
    "                \n",
    "                # Load state and outputs\n",
    "                x_t = torch.from_numpy(sample['x_t'][:]).float()\n",
    "                lambda_t_plus_1 = torch.from_numpy(sample['lambda_t_plus_1'][:]).float()\n",
    "                dx_dt_lambda = torch.from_numpy(sample['dx_dt_lambda'][:]).float()\n",
    "                \n",
    "                data.append((params, x_t, lambda_t_plus_1, dx_dt_lambda))\n",
    "            return data\n",
    "        \n",
    "        # Load training and testing datasets\n",
    "        train_data = load_dataset(f['train'])\n",
    "        test_data = load_dataset(f['test'])\n",
    "    \n",
    "    return {\n",
    "        'train': train_data,\n",
    "        'metadata': metadata\n",
    "    }, {\n",
    "        'test': test_data,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "class SavedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pre-saved data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "nr,nc=10,10\n",
    "\n",
    "\"\"\"\n",
    "Time to generate the dataset and train the model\n",
    ": 3s\n",
    "\n",
    "generate_and_save_dataset(\n",
    "    filename='linear_model_data.h5',\n",
    "    num_train_samples=180,\n",
    "    num_test_samples=20,\n",
    "    nr=10,\n",
    "    nc=10,\n",
    "    dt=0.1,\n",
    "    F=1.0\n",
    ")\"\"\"\n",
    "\"\"\"\n",
    "generate_and_save_dataset(\n",
    "    filename='linear_model_data.h5',\n",
    "    num_train_samples=180000,\n",
    "    num_test_samples=20000,\n",
    "    nr=10,\n",
    "    nc=10,\n",
    "    dt=0.1,\n",
    "    F=1.0\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do:\n",
    "\n",
    "- Set things up so we can save data\n",
    "- Train with more data to see if it can learn effectively\n",
    "\n",
    "- Try covariance (maybe test model on OOD covariance?)\n",
    "- Try data from a model simulated over time\n",
    "- See how well that does on backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate its performance with visualization\n",
    "    \"\"\"\n",
    "    # Training setup\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # Lists to store losses for plotting\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for params, x_t, lambda_t_plus_1, dx_dt_lambda in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_t, lambda_t_plus_1, params)\n",
    "            loss = criterion(output, dx_dt_lambda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for params, x_t, lambda_t_plus_1, dx_dt_lambda in test_dataloader:\n",
    "                output = model(x_t, lambda_t_plus_1, params)\n",
    "                test_loss = criterion(output, dx_dt_lambda)\n",
    "                total_test_loss += test_loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        \n",
    "        # Store losses for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:  # Print every 10 epochs\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "    plt.title('Training and Testing Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test examples\n",
    "    model.eval()\n",
    "    test_params, test_x, test_lambda, test_true = next(iter(test_dataloader))\n",
    "    \n",
    "    # Get model prediction for first test example\n",
    "    with torch.no_grad():\n",
    "        predicted = model(test_x[0:1], test_lambda[0:1], \n",
    "                        {k: v[0:1] for k, v in test_params.items()})\n",
    "    \n",
    "    # Remove batch dimension\n",
    "    predicted = predicted.squeeze(0)\n",
    "    test_true = test_true[0]  # Get first example's true values\n",
    "    \n",
    "    # Print comparison\n",
    "    print(f\"\\nComparison of first 10 values (from test set):\")\n",
    "    print(f\"{'Index':<6} {'True Value':>12} {'Predicted':>12} {'Difference':>12}\")\n",
    "    print(\"-\" * 44)\n",
    "    for i in range(10):\n",
    "        print(f\"{i:<6} {test_true[i]:>12.6f} {predicted[i]:>12.6f} {(test_true[i] - predicted[i]):>12.6f}\")\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(f\"\\nTest Set Statistics:\")\n",
    "    print(f\"Mean Absolute Error: {torch.abs(test_true - predicted).mean():.6f}\")\n",
    "    print(f\"Root Mean Square Error: {torch.sqrt(torch.mean((test_true - predicted)**2)):.6f}\")\n",
    "    print(f\"True vector norm: {torch.norm(test_true):.6f}\")\n",
    "    print(f\"Predicted vector norm: {torch.norm(predicted):.6f}\")\n",
    "    \n",
    "    # Plot true vs predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(test_true.numpy(), label='True Values', marker='o')\n",
    "    plt.plot(predicted.numpy(), label='Predicted Values', marker='x')\n",
    "    plt.title('True vs Predicted Values (Test Set)')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /train data: 100%|██████████| 180000/180000 [09:23<00:00, 319.55it/s]\n",
      "Loading /test data: 100%|██████████| 20000/20000 [01:36<00:00, 208.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 1280\n",
      "Training started...\n",
      "Epoch 10/600\n",
      "Training Loss: 169.3921\n",
      "Test Loss: 169.7578\n",
      "Epoch 20/600\n",
      "Training Loss: 168.7892\n",
      "Test Loss: 169.2647\n",
      "Epoch 30/600\n",
      "Training Loss: 166.2119\n",
      "Test Loss: 166.7372\n",
      "Epoch 40/600\n",
      "Training Loss: 165.8158\n",
      "Test Loss: 166.4536\n",
      "Epoch 50/600\n",
      "Training Loss: 165.6289\n",
      "Test Loss: 166.3011\n",
      "Epoch 60/600\n",
      "Training Loss: 165.4453\n",
      "Test Loss: 165.9948\n",
      "Epoch 70/600\n",
      "Training Loss: 165.3685\n",
      "Test Loss: 165.9264\n",
      "Epoch 80/600\n",
      "Training Loss: 165.3003\n",
      "Test Loss: 166.0233\n",
      "Epoch 90/600\n",
      "Training Loss: 165.2725\n",
      "Test Loss: 165.9051\n",
      "Epoch 100/600\n",
      "Training Loss: 165.2612\n",
      "Test Loss: 166.0332\n",
      "Epoch 110/600\n",
      "Training Loss: 165.2113\n",
      "Test Loss: 165.9668\n",
      "Epoch 120/600\n",
      "Training Loss: 165.2123\n",
      "Test Loss: 165.9913\n",
      "Epoch 130/600\n",
      "Training Loss: 165.1555\n",
      "Test Loss: 165.9623\n",
      "Epoch 140/600\n",
      "Training Loss: 165.1470\n",
      "Test Loss: 166.1304\n",
      "Epoch 150/600\n",
      "Training Loss: 165.1412\n",
      "Test Loss: 166.2851\n",
      "Epoch 160/600\n",
      "Training Loss: 165.1010\n",
      "Test Loss: 166.1456\n",
      "Epoch 170/600\n",
      "Training Loss: 165.0623\n",
      "Test Loss: 166.0524\n",
      "Epoch 180/600\n",
      "Training Loss: 165.0392\n",
      "Test Loss: 166.1246\n",
      "Epoch 190/600\n",
      "Training Loss: 165.0058\n",
      "Test Loss: 166.2066\n",
      "Epoch 200/600\n",
      "Training Loss: 164.9773\n",
      "Test Loss: 166.0680\n",
      "Epoch 210/600\n",
      "Training Loss: 164.9458\n",
      "Test Loss: 166.1109\n",
      "Epoch 220/600\n",
      "Training Loss: 164.9396\n",
      "Test Loss: 166.3038\n",
      "Epoch 230/600\n",
      "Training Loss: 164.8885\n",
      "Test Loss: 166.2295\n",
      "Epoch 240/600\n",
      "Training Loss: 164.8712\n",
      "Test Loss: 166.2848\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "nr,nc=10,10\n",
    "train_data, test_data = load_dataset('linear_model_data.h5')\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = SavedDataset(train_data['train'])\n",
    "test_dataset = SavedDataset(test_data['test'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=320, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=320, shuffle=False)\n",
    "\n",
    "model = LinearModelNet(nr, nc)\n",
    "trained_model, train_losses, test_losses = train_and_evaluate_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    num_epochs=600,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
