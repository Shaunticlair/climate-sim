{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Variations\n",
    "\n",
    "In this notebook, we explore different variants of gradient descent to optimize our atmosphere.\n",
    "\n",
    "- These variants start simple, and become more complex, in order to more accurately represent the atmosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import helper\n",
    "\n",
    "run_all_code = False # Runs code blocks below, one by one, with plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate data:\n",
    "\n",
    "- The real data, using the real atmospheric forcing\n",
    "- Fake data, using our noisy atmospheric forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr,nc = 32,32\n",
    "dt = 0.01\n",
    "F = 0.1\n",
    "\n",
    "#C_known is the covariance matrix of the full control vector f\n",
    "\n",
    "C_control, x0, _, M = helper.generate_world(nr, nc, dt, F)\n",
    "\n",
    "#C_known is the covariance matrix of the correct part of the control vector f\n",
    "#C_error is the covariance matrix of the incorrect part of the control vector f\n",
    "\n",
    "gamma = 2/3\n",
    "C_known = C_control * gamma\n",
    "C_error = C_control * (1-gamma)\n",
    "C_ocean = C_control / 6\n",
    "\n",
    "f_true, f_guess = helper.generate_true_and_first_guess_field(C_known, C_error, nr, nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atmosphere forcing coefficient\n",
    "F = 0.1\n",
    "# Standard deviation of the noise in the observations\n",
    "sigma = 0.1 \n",
    "\n",
    "\n",
    "# Number of timesteps to run the simulation for (repeated each iter)\n",
    "num_timesteps = 4\n",
    "# Number of iterations of gradient descent (repeated each run)\n",
    "num_iters = 5\n",
    "# Step size for gradient descent\n",
    "step_size = 0.1\n",
    "\n",
    "# Number of times to run the whole gradient descent optimization\n",
    "num_runs = 1\n",
    "\n",
    "# Run the simulation with the true and guessed control vector\n",
    "saved_timesteps, real_state_over_time  = helper.compute_affine_time_evolution_simple(x0, M, F*f_true,  num_timesteps)\n",
    "saved_timesteps, guess_state_over_time = helper.compute_affine_time_evolution_simple(x0, M, F*f_guess, num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we'll need observations of the real ocean state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_per_timestep = 50\n",
    "\n",
    "\n",
    "\n",
    "observed_state_over_time_2d = helper.observe_over_time(real_state_over_time, sigma, \n",
    "                                                       num_obs_per_timestep, nr, nc)\n",
    "\n",
    "\n",
    "observed_state_over_time =     [np.reshape(observed_state_2d, (nr*nc, 1)) \n",
    "                                for observed_state_2d in observed_state_over_time_2d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concept for this notebook: Gradient Descent\n",
    "\n",
    "Our goal is to improve the accuracy of our atmosphere $f$ by modifying it, so that it creates a more accurate ocean simulation over time, $x(t)$.\n",
    "\n",
    "- We measure the accuracy of our ocean simulation using the above observations, and plugging them into a loss function, $J(x)$.\n",
    "\n",
    "  - $J(x)$ tells us how *bad* the fit between our observations and our simulation are.\n",
    "\n",
    "- So, we want to choose an updated $f$ that minimizes $J$.\n",
    "\n",
    "To accomplish this, we'll use **gradient descent**: we take small steps, modifying $f$ using gradient $-dJ/df$ repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventions for discussing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the following sections, our work can get pretty messy. So, the ease the process, we introduce some symbolic conventions for discussing each component of our algorithm:\n",
    "\n",
    "- $i$: The iteration of gradient descent we're referencing, starting from $i=0$.\n",
    "\n",
    "- $f_i$: The total atmospheric control vector at the start of iteration $i$.\n",
    "\n",
    "- $u_i$: The update we apply to $f_i$ during iteration $i$:\n",
    "\n",
    "$$f_{i+1} = f_i + u_i$$\n",
    "\n",
    "- $a_i$: The total atmospheric adjustment we've made to $f_0$ before iteration $i$.\n",
    "\n",
    "$$f_i = f_0 + a_i$$\n",
    "\n",
    "We'll generally refer to $f_{true}$ as the true atmospheric control vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradient Descent + Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try implementing the most basic version of gradient descent:\n",
    "\n",
    "- $\\eta$ is our \"step size\".\n",
    "\n",
    "$$ u_i = - \\eta \\frac{dJ}{df}$$\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "While we're at it, we'll keep track of three types of \"loss\":\n",
    "\n",
    "- \"Ocean loss\": this is the misfit between the ocean simulation $x(t)$, and our observed state $Hy(t)$. This is $J$.\n",
    "\n",
    "  - We mask/ignore cells that aren't observed.\n",
    "\n",
    "$$ J_{ocean} = J = \\sum_t \\Big(x - Hy(t) \\Big)^\\top \\Big( x - Hy(t) \\Big)$$\n",
    "\n",
    "- \"Atmosphere loss\": this is the misfit between the current guess for the atmospheric control $f_i$, and what we know is the true atmospheric control $f_{true}$.\n",
    "\n",
    "  - This measure is not used for optimization: rather, we use it to check how well our gradient descent is training on information it doesn't have ($f_{true}$). Thus, we can use this as a proxy for overfitting.\n",
    "\n",
    "  $$J_{atm} = \\Big(f_{true} - f_i \\Big)^\\top \\Big( f_{true} - f_i \\Big)$$\n",
    "\n",
    "- \"Mahalanobis loss\": this is a measure of how far the covariance of the control adjustment $a_i$ is to a given covariance matrix $C$ (the \"true\" covariance). \n",
    "\n",
    "  - It becomes larger if covariance of $a_i$ moves further away from $C$, or the $|a_i|$ increases.\n",
    "\n",
    "  - Later, we want to optimize $a_i$ to have covariance $C$, so this will be a useful metric.\n",
    "\n",
    "  - It also gives an idea of how \"statistically accurate\" $a_i$ is.\n",
    "\n",
    "$$J_{mhl} = a_i^\\top C^{-1} a_i$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a helper function to keep calculation of our losses separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_template = { #Losses at each iteration\n",
    "        \"ocean_misfit\": [],\n",
    "        \"atmosphere_misfit\": [],\n",
    "        \"mahalanobis(covariance similarity)\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def update_losses(losses, ocean_states_observed, ocean_states_simulated, f_guess, f_adjust, f_true, C_error):\n",
    "    \"\"\"\n",
    "    Updates the losses dictionary with new loss values for ocean, atmosphere, and control adjustment.\n",
    "\n",
    "    Args:\n",
    "    losses (dict): Dictionary containing lists of loss values\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    ocean_states_simulated (list): List of simulated ocean states\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    f_adjust (np.ndarray): Adjustment to the atmospheric forcing field\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated losses dictionary with new loss values appended\n",
    "    \"\"\"\n",
    "    f_i = f_guess + f_adjust\n",
    "\n",
    "    # Compute losses\n",
    "    ocean_loss_i = helper.compute_J(ocean_states_observed, ocean_states_simulated) # J_{ocean} = J\n",
    "    atmos_loss_i = helper.compute_Jt(f_true, f_i)                                  # J_{atm} = misfit of atm\n",
    "    \n",
    "    mahal = f_adjust.T @ np.linalg.inv(C_error) @ f_adjust # C_error should be the covariance of our adjustment\n",
    "    mahal_loss_i = np.linalg.norm(mahal)\n",
    "\n",
    "    #Store losses\n",
    "    losses[\"ocean_misfit\"].append(ocean_loss_i)\n",
    "    losses[\"atmosphere_misfit\"].append(atmos_loss_i)\n",
    "    losses[\"mahalanobis(covariance similarity)\"].append(mahal_loss_i)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_simple(M, F, f_true, f_guess, C_known, C_error,         # World parameters\n",
    "                              x0, num_timesteps,                                    # Simulation parameters\n",
    "                              ocean_states_observed, num_iters, step_size,      # Optimization parameters\n",
    "                              disp=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the atmospheric forcing field. \n",
    "    \n",
    "    The step we take at each iteration is computed using -step_size*dJ/df.\n",
    "\n",
    "    Args:\n",
    "        M: Model matrix\n",
    "        F: Scalar constant for forcing\n",
    "        f_true: True atmospheric forcing field\n",
    "        f_guess: Initial guess for the atmospheric forcing field\n",
    "        C_known: Covariance matrix for the known portion of the control\n",
    "        C_error: Covariance matrix for the control error\n",
    "        x0: Initial ocean state\n",
    "        num_timesteps: Number of timesteps\n",
    "        ocean_states_observed: Observed ocean states\n",
    "        num_iters: Number of iterations\n",
    "        step_size: Step size for gradient descent\n",
    "        step_compute_method: Function to compute the step to take at each iteration\n",
    "        extra_params: Extra parameters to pass to the step\n",
    "        disp: Flag to print information\n",
    "\n",
    "    Returns:\n",
    "        f: Optimized atmospheric forcing field\n",
    "        losses: Dictionary of losses\n",
    "            ocean_misfit: Ocean loss at each iteration\n",
    "            atmosphere: Atmospheric loss at each iteration\n",
    "            mahalanobis(covariance similarity): Mahalanobis distance for the control adjustment at each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    f_adjust = np.zeros((nr*nc,1))\n",
    "\n",
    "    losses = losses_template.deepcopy()\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        if i%10==0 and disp:\n",
    "            print(\"Iteration\", i)\n",
    "\n",
    "        #Compute results of previous update rule\n",
    "\n",
    "        f_i = f_guess + f_adjust #f_i = f_0 + a_i\n",
    "        _, ocean_states_simulated = helper.compute_affine_time_evolution(x0, M, F*f_i, num_timesteps)\n",
    "\n",
    "        # Compute and store losses\n",
    "        losses = update_losses(losses, ocean_states_observed, ocean_states_simulated, f_guess, f_adjust, f_true, C_error)\n",
    "\n",
    "        # Apply update rule (gradient descent step)\n",
    "        s = helper.compute_dJ_df(M, F, ocean_states_observed, ocean_states_simulated)\n",
    "        u_i = -step_size * s #Update rule\n",
    "\n",
    "        f_adjust = f_adjust + u_i\n",
    "\n",
    "    return f_adjust, losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic form of our update rule for $u_i$. However, we want to explore other possible update rules. So, we'll create a boilerplate version of gradient descent, that allows us to choose our update rule, without re-writing the rest of our code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we'll need to take care of some book-keeping: debug variables that we may find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_debug_vars = { #Debug variables to compute at each iteration\n",
    "    \"Norm of s_i\": [],\n",
    "    \"Expected Delta J w simple gd\": [],\n",
    "    \"Expected Delta J w update rule\": [],\n",
    "    \"Norm of simple gd ui\": [],\n",
    "    \"Norm of update rule ui\": [],\n",
    "    \"Normalized dot product $a_i$ and $u_i$\": [],\n",
    "    \"$s_i^T Cs_i$\": [],\n",
    "    \"$a_i$\": [],\n",
    "    \"Normalized $a_i^T s_i$\": [],\n",
    "    \"Normalized $-Cs_i \\cdot a_i$\": [],\n",
    "    \"Norm of $a_i$\": [],\n",
    "    \"Actual Delta J\": []\n",
    "}\n",
    "\n",
    "def update_debug_vars(debug_vars, x0, M, F, f_guess, f_adjust, \n",
    "                      C_known, C_error, \n",
    "                      s, step_size, ui, \n",
    "                      ocean_states_simulated, ocean_states_observed):\n",
    "    \"\"\"\n",
    "    Updates the debug variables dictionary with various metrics for gradient descent analysis.\n",
    "\n",
    "    Args:\n",
    "    debug_vars (dict): Dictionary containing lists of debug variable values\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    f_adjust (np.ndarray): Adjustment to the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    ui (np.ndarray): Update vector for the current iteration\n",
    "    ocean_states_simulated (list): List of simulated ocean states\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated debug_vars dictionary with new values appended to each metric\n",
    "    \"\"\"\n",
    "    #Initialize useful variables\n",
    "    num_timesteps = len(ocean_states_observed)\n",
    "    ui_simple_gd = -step_size * s\n",
    "    delta = s.T @(ui_simple_gd)\n",
    "\n",
    "    #Compute debug vars\n",
    "    norm_s = np.linalg.norm(s)\n",
    "    exp_delta_J_simple_gd = (s.T @ ui_simple_gd)[0,0]\n",
    "    exp_delta_J_update_rule = (s.T @ ui)[0,0]\n",
    "    norm_simple_ui = np.linalg.norm(ui_simple_gd)\n",
    "    norm_ui = np.linalg.norm(ui)\n",
    "    norm_dot_product = (f_adjust.T @ ui)[0,0] / (np.linalg.norm(f_adjust) * np.linalg.norm(ui))\n",
    "    sTCs = (s.T @ C_error @ s)[0,0]\n",
    "    ai = f_adjust\n",
    "    normalized_aiTs = (f_adjust.T @ s)[0,0] / (np.linalg.norm(f_adjust) * np.linalg.norm(s))\n",
    "    normalized_Csdotai = (f_adjust.T @ (C_error @ s))[0,0] / (np.linalg.norm(f_adjust) * np.linalg.norm(C_error @ s))\n",
    "    norm_ai = np.linalg.norm(f_adjust)\n",
    "\n",
    "    #Store debug vars\n",
    "    debug_vars[\"Norm of s_i\"].append(norm_s)\n",
    "    debug_vars[\"Expected Delta J w simple gd\"].append(exp_delta_J_simple_gd)\n",
    "    debug_vars[\"Expected Delta J w update rule\"].append(exp_delta_J_update_rule)\n",
    "    debug_vars[\"Norm of simple gd ui\"].append(norm_simple_ui)\n",
    "    debug_vars[\"Norm of update rule ui\"].append(norm_ui)\n",
    "    debug_vars[\"Normalized dot product $a_i$ and $u_i$\"].append(norm_dot_product)\n",
    "    debug_vars[\"$s_i^T Cs_i$\"].append(sTCs)\n",
    "    debug_vars[\"$a_i$\"].append(ai)\n",
    "    debug_vars[\"Normalized $a_i^T s_i$\"].append(normalized_aiTs)\n",
    "    debug_vars[\"Normalized $-Cs_i \\cdot a_i$\"].append(-normalized_Csdotai)\n",
    "    debug_vars[\"Norm of $a_i$\"].append(norm_ai)\n",
    "\n",
    "    #Handle last debug var: Actual Delta J\n",
    "    f_new = f_guess + f_adjust + ui\n",
    "    _, new_ocean_states_simulated = helper.compute_affine_time_evolution_simple(x0, M, F*f_new, num_timesteps)\n",
    "\n",
    "    new_J = helper.compute_J(ocean_states_observed, new_ocean_states_simulated)\n",
    "    old_J = helper.compute_J(ocean_states_observed, ocean_states_simulated)\n",
    "\n",
    "    actual_delta_J = new_J - old_J\n",
    "    \n",
    "    debug_vars[\"Actual Delta J\"].append(actual_delta_J)\n",
    "    \n",
    "    return debug_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can actually implement our template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_template(M, F, f_true, f_guess, C_known, C_error,         # World parameters\n",
    "                              x0, num_timesteps,                                    # Simulation parameters\n",
    "                              ocean_states_observed, num_iters, step_size,      # Optimization parameters\n",
    "                              update_rule, update_params, disp=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the atmospheric forcing field. \n",
    "    \n",
    "    The step we take at each iteration is computed using a modified update rule, and extra parameters as necessary.\n",
    "\n",
    "    Args:\n",
    "        M: Model matrix\n",
    "        F: Scalar constant for forcing\n",
    "        f_true: True atmospheric forcing field\n",
    "        f_guess: Initial guess for the atmospheric forcing field\n",
    "        x0: Initial ocean state\n",
    "        num_timesteps: Number of timesteps\n",
    "        ocean_states_observed: Observed ocean states\n",
    "        num_iters: Number of iterations\n",
    "        step_size: Step size for gradient descent\n",
    "        C_known: Covariance matrix for the known portion of the control\n",
    "        C_error: Covariance matrix for the control error\n",
    "        update_params: Function to compute the update rule to take at each iteration\n",
    "        extra_params: Extra parameters to pass to the update rule\n",
    "        disp: Flag to print information\n",
    "\n",
    "    Returns:\n",
    "        f: Optimized atmospheric forcing field\n",
    "        losses: Dictionary of losses\n",
    "            ocean_misfit: Ocean loss at each iteration\n",
    "            atmosphere: Atmospheric loss at each iteration\n",
    "            mahalanobis(covariance similarity): Mahalanobis distance for the control adjustment at each iteration\n",
    "    \"\"\"\n",
    "    size = f_guess.shape[0]\n",
    "    f_adjust = np.zeros((size,1))\n",
    "\n",
    "    losses = copy.deepcopy(losses_template)\n",
    "    debug_vars = copy.deepcopy(possible_debug_vars)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        if i%10==0 and disp:\n",
    "            print(\"Iteration\", i)\n",
    "        \n",
    "        #Compute results of previous update rule\n",
    "        f_i = f_guess + f_adjust #f_i = f_0 + a_i\n",
    "        _, ocean_states_simulated = helper.compute_affine_time_evolution_simple(x0, M, F*f_i, num_timesteps)\n",
    "        \n",
    "\n",
    "        # Compute and store losses\n",
    "        losses = update_losses(losses, ocean_states_observed, ocean_states_simulated, f_guess, f_adjust, f_true, C_error)\n",
    "        \n",
    "        # Compute and store debug variables\n",
    "        \n",
    "        s = helper.compute_dJ_df(M, F, ocean_states_observed, ocean_states_simulated)\n",
    "        ui = update_rule(i, s, step_size, f_adjust, *update_params) #Update rule\n",
    "\n",
    "        debug_vars = update_debug_vars(debug_vars, x0, M, F, f_guess, f_adjust, \n",
    "                                       C_known, C_error, \n",
    "                                       s, step_size, ui, \n",
    "                                       ocean_states_simulated, ocean_states_observed)\n",
    "\n",
    "        # Apply update rule to f_adjust\n",
    "\n",
    "        f_adjust = f_adjust + ui\n",
    "\n",
    "        \n",
    "\n",
    "    return f_adjust, losses, debug_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to reproduce simple gradient descent using this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gradient_update_rule(curr_iter, s, step_size, f_adjust):\n",
    "    \"\"\"\n",
    "    Computes the update step for simple gradient descent.\n",
    "\n",
    "    Args:\n",
    "    curr_iter (int): Current iteration number (unused in this function)\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    f_adjust (np.ndarray): Current adjustment to the forcing field (unused in this function)\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Update step for the forcing field adjustment\n",
    "    \"\"\"\n",
    "    return -step_size * s  # Just use the gradient of the loss\n",
    "\n",
    "def simple_gradient_descent(M, F, f_true, f_guess, C_known, C_error,       # World parameters\n",
    "                            x0, timesteps,                                   # Simulation parameters\n",
    "                            ocean_states_observed, num_iters, step_size,      # Optimization parameters\n",
    "                            disp=False):                                      # Optimization method\n",
    "    \"\"\"\n",
    "    Implements simple gradient descent for optimizing the atmospheric forcing field.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (f_adjust, losses, debug_vars)\n",
    "        f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "        losses (dict): Dictionary of loss values over iterations\n",
    "        debug_vars (dict): Dictionary of debug variables over iterations\n",
    "    \"\"\"\n",
    "    return gradient_descent_template(M, F, f_true, f_guess, C_known, C_error, \n",
    "                                     x0, timesteps, \n",
    "                                     ocean_states_observed, num_iters, step_size,\n",
    "                                     simple_gradient_update_rule, [], disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our Adjustments Using Covariance Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Motivation\n",
    "\n",
    "Our current approach works under the assumption that, if our atmosphere adjustment creates a more realistic ocean simulation, then it creates a more realistic atmosphere.\n",
    "\n",
    "- But it's possible for the atmosphere to induce the expected ocean currents, while being unrealistic.\n",
    "- In fact, there may be many ways to create an atmosphere that gives the desired effect on ocean state.\n",
    "\n",
    "### Our Solution\n",
    "\n",
    "So, we want to enforce another layer of realism on our adjustment: in addition to improving our ocean simulation, we also want the adjustment to have similar *structure* to what we expect from the atmosphere.\n",
    "\n",
    "- We'll do this by modifying our adjustment, so that the **covariance** is similar to what we expect.\n",
    "- If our adjustment has a similar covariance to the overall atmosphere, we hope it'll be a more realistic adjustment. \n",
    "\n",
    "We want to experiment with four different ways we can implement this covariance enforcement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cholesky Approach\n",
    "\n",
    "1. Try to force our update $u_i$ to have covariance $C$, using the cholesky decomposition.\n",
    "      - This is the way we generated our random vector with covariance $C$, previously.\n",
    "\n",
    "$$ LL^\\top = C \\qquad \\qquad z \\sim \\mathcal{N}(0,1) \\qquad \\qquad Lz \\sim \\mathcal{N}(0,C)$$\n",
    "\n",
    "So, our result would be the new update rule $u_i$.\n",
    "\n",
    "$$ z = -\\eta\\frac{dJ}{df} \\qquad \\qquad \\qquad u_i = Lz $$\n",
    "\n",
    "The problem with this approach, is that the proof for Cholesky assumes that $z$ is a random vector: in our case, we want to modify $\\frac{dJ}{df}$, which is not a random vector. Maybe it'll have the same smoothing effect, but it doesn't have the same theoretical grounding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Include Covariance Constraint in $J$\n",
    "\n",
    "2. Add the covariance constraint (penalize mahalanobis distance) to $a_i$ in our loss function, while generating $dJ/df$. \n",
    "\n",
    "- We'll include a weight $\\alpha$ to determine the relative importance of the second term.\n",
    "\n",
    "$$J'(x, f) = J(x) +\n",
    "            \\alpha \\Big( a^\\top C^{-1} a \\Big) $$\n",
    "\n",
    "This approach should, hypothetically, encourage our total control adjustment $a_i$ to have covariance $C$.\n",
    "\n",
    "If we use this $J'$ and then use gradient descent, we get:\n",
    "\n",
    "$$u_i = -\\eta \\cdot \\Bigg( \\frac{dJ}{df} + 2\\alpha C^{-1} a  \\Bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dan's Method\n",
    "\n",
    "Compute $dJ/df$ first, *then* constrain your update rule $u_i$ to do what we want:\n",
    "\n",
    "- $u_i$ should reduce $J$, in order to make a better fit to ocean date. We'll say that it needs to decrease by $\\delta$. We can approximate this as:\n",
    "\n",
    "$$ \\delta \\approx s^\\top u_i$$\n",
    "\n",
    "- We also want to keep the covariance of the update rule closer to $C$: we'll penalize the mahalanobis distance.\n",
    "\n",
    "$$u_i^\\top C^{-1} u_i $$\n",
    "\n",
    "Together, this can be encoded by the lagrangian (converting $u)\n",
    "\n",
    "$$ \\mathcal{L} = \\lambda (\\delta - s^\\top u_i) + u_i^\\top C^{-1} u_i$$\n",
    "\n",
    "If we solve for the minimizing conditions $d\\mathcal{L}/du_i=d\\mathcal{L}/d\\lambda = 0$, we find a solution:\n",
    "\n",
    "$$ u_i = \\delta \\Big( \\frac{Cs}{s^\\top C s} \\Big)$$\n",
    "\n",
    "This is the proposal by Dan Amrhein, in his informal paper, \"Ocean state estimation with atmospheric adjustments constrained by prior covariance and observations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dan's Method Modified\n",
    "\n",
    "One problem: originally, the goal was to pressure our total control adjustment $a_i$ to have covariance $C$. But Dan's method, when applied to gradient descent, instead pressures each update $u_i$ to have covariance $C$.\n",
    "\n",
    "- If we keep adding updates of covariance $C$ to our control adjustment, the covariance will keep increasing.\n",
    "\n",
    "So, we'll modify Dan's approach:\n",
    "\n",
    "- Compute $dJ/df$ first, then constrain your **total control adjustment** $u_i + a_i$ to have covariance $C$, while making sure your update $u_i$ reduces $J$.\n",
    "\n",
    "$$ \\mathcal{L} = \\lambda (\\delta - s^\\top u_i) + (a_i+u_i)^\\top C^{-1} (a_i+u_i)$$\n",
    "\n",
    "If we solve for $d\\mathcal{L}/du_i=d\\mathcal{L}/d\\lambda = 0$, we find a solution:\n",
    "\n",
    "$$u_i = -a_i + \\Big(\\delta + a_i^\\top s\\Big) \\Big( \\frac{ Cs }{s^\\top C s} \\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing and testing covariance-enforcement methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cholesky Approach\n",
    "\n",
    "First, we'll take the simplest approach: compute the derivative, and then apply the covariance adjustment $z \\to Lz$, as if we had a random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cholesky_update_rule(curr_iter, s, step_size, f_adjust, C_error):\n",
    "    \"\"\"\n",
    "    Computes the update step using Cholesky decomposition of the error covariance matrix.\n",
    "\n",
    "    Args:\n",
    "    curr_iter (int): Current iteration number (unused in this function)\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    f_adjust (np.ndarray): Current adjustment to the forcing field (unused in this function)\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Update step for the forcing field adjustment\n",
    "\n",
    "    Notes:\n",
    "    Applies the Cholesky decomposition L of C_error to s, then rescales the result \n",
    "    to match the original gradient's magnitude.\n",
    "    \"\"\"\n",
    "    # Apply cholesky decomposition L : C_error = L @ L.T\n",
    "    L = np.linalg.cholesky(C_error)\n",
    "    cholesky_s = L @ s\n",
    "\n",
    "    # Rescale so magnitude is the same\n",
    "    rescaled_cholesky_s = cholesky_s * (np.linalg.norm(s) / np.linalg.norm(cholesky_s))\n",
    "    step = - step_size * rescaled_cholesky_s\n",
    "\n",
    "    return step\n",
    "\n",
    "def cholesky_gradient_descent(M, F, f_true, f_guess, C_known, C_error,       # World parameters\n",
    "                              x0, timesteps,                                 # Simulation parameters\n",
    "                              ocean_states_observed, num_iters, step_size,   # Optimization parameters\n",
    "                              disp=False):                                   # Optimization method\n",
    "    \"\"\"\n",
    "    Implements gradient descent using Cholesky decomposition for optimizing the atmospheric forcing field.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (f_adjust, losses, debug_vars)\n",
    "        f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "        losses (dict): Dictionary of loss values over iterations\n",
    "        debug_vars (dict): Dictionary of debug variables over iterations\n",
    "    \"\"\"\n",
    "    return gradient_descent_template(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                     x0, timesteps, \n",
    "                                     ocean_states_observed, num_iters, step_size,\n",
    "                                     cholesky_update_rule, [C_error], disp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if run_all_code:\n",
    "\n",
    "    f_adjust_cholesky, losses_cholesky, debug_vars_cholesky = cholesky_gradient_descent(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                                                                        x0, num_timesteps, \n",
    "                                                                                        observed_state_over_time, num_iters, step_size, \n",
    "                                                                                        disp=True)\n",
    "\n",
    "\n",
    "    # Plot the true field, the first guess, and the improved field\n",
    "    f_optimized_cholesky = f_guess + f_adjust_cholesky\n",
    "\n",
    "    many_states = [f_true, f_guess, f_optimized_cholesky]\n",
    "    titles = ['True field', 'First-guess field', 'Cholesky field']\n",
    "    big_title = 'Atmospheric Field: Gradient Descent with Cholesky Decomposition'\n",
    "\n",
    "    helper.plot_multi_heatmap(many_states, nr, nc, titles, big_title, vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to be able to directly compare the two approaches.\n",
    "- In order to make an apples-to-apples comparison, both gradient descent methods have to be using the same initialization, model, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gd_methods_once(M, F, f_true, f_guess, C_known, C_error, \n",
    "                            x0, timesteps, \n",
    "                            ocean_states_observed, num_iters, step_size, \n",
    "                            methods, disp=False):\n",
    "    \"\"\"\n",
    "    Runs multiple methods of gradient descent on the same dataset and compares their performance.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    methods (list): List of gradient descent methods to compare. Each method is a list of the form\n",
    "                    [\"Method Name\", method_func, extra_params]\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are method names and values are tuples containing:\n",
    "          - f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "          - losses (dict): Dictionary of loss values over iterations\n",
    "          - debug_vars (dict): Dictionary of debug variables over iterations\n",
    "\n",
    "    Notes:\n",
    "    This function applies each specified gradient descent method to the same initial conditions\n",
    "    and dataset, allowing for direct comparison of their performance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for method_name, method_func, extra_params in methods:\n",
    "        if disp:\n",
    "            print(f\"Running method {method_name}\")\n",
    "\n",
    "        f_adjust, losses, debug_vars = gradient_descent_template(M, F, f_true, f_guess, C_known, C_error, \n",
    "                                                     x0, timesteps, \n",
    "                                                     ocean_states_observed, num_iters, step_size,\n",
    "                                                     method_func, extra_params, disp)\n",
    "        results[method_name] = (f_adjust, losses, debug_vars)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "\n",
    "    methods = [\n",
    "        [\"Simple Gradient Descent\", simple_gradient_update_rule, []],\n",
    "        [\"Cholesky Gradient Descent\", cholesky_update_rule, [C_error]],\n",
    "    ]\n",
    "\n",
    "    # Run multiple methods of gradient descent\n",
    "    results = compare_gd_methods_once(M, F, f_true, f_guess, C_known, C_error, \n",
    "                                    x0, num_timesteps, \n",
    "                                    observed_state_over_time, num_iters, step_size, \n",
    "                                    methods, disp=True)\n",
    "\n",
    "    # Plot the results \n",
    "    many_states = [f_true] + [f_guess + f_adjust for f_adjust, _, _ in results.values()]\n",
    "    titles = ['True field'] + [f\"{method[0]} field\" for method in methods]\n",
    "    big_title = 'Atmospheric Field: Gradient Descent with Different Methods'\n",
    "\n",
    "    helper.plot_multi_heatmap(many_states, nr, nc, titles, big_title, vmin=None, vmax=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might not get a representative sample from only running each once. So, we'll create a function for running it multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gd_methods_many_times(nr, nc, dt, F, gamma, sigma, num_obs_per_timestep, \n",
    "                                  num_timesteps, num_iters, step_size, \n",
    "                                  C_known, C_error, methods, num_runs, disp=False):\n",
    "    \"\"\"\n",
    "    Create many different sets of data. \n",
    "    For each one, we will run each of our gradient descent methods.\n",
    "    Once we finish, we average losses across all runs.\n",
    "\n",
    "    Args:\n",
    "    nr (int): Number of rows in the grid\n",
    "    nc (int): Number of columns in the grid\n",
    "    dt (float): Time step\n",
    "    F (float): Forcing parameter\n",
    "    gamma (float): Proportion of the control vector that is correct\n",
    "    sigma (float): Standard deviation of observation noise\n",
    "    num_obs_per_timestep (int): Number of observations per timestep\n",
    "    num_timesteps (int): Number of timesteps\n",
    "    num_iters (int): Number of iterations of gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    methods (list): List of gradient descent methods to compare\n",
    "    num_runs (int): Number of times to run the whole optimization process\n",
    "    disp (bool): If True, print progress\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing averaged losses and debug variables for each method\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize results dictionary\n",
    "    losses =     {method[0]: copy.deepcopy(losses_template) \n",
    "                for method in methods}\n",
    "    debug_vars = {method[0]: copy.deepcopy(possible_debug_vars) \n",
    "                  for method in methods}\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        if disp:\n",
    "            print(f\"Run {run + 1}/{num_runs}\")\n",
    "\n",
    "        # Generate world\n",
    "        C_control, x0, _, M = helper.generate_world(nr, nc, dt, F)\n",
    "        C_known, C_error = C_control * gamma, C_control * (1-gamma)\n",
    "        \n",
    "        f_true, f_guess = helper.generate_true_and_first_guess_field(C_known, C_error, nr, nc)\n",
    "\n",
    "        # Run the simulation with the true and guessed control vector\n",
    "        _, real_state_over_time  = helper.compute_affine_time_evolution_simple(x0, M, F*f_true,  num_timesteps)\n",
    "\n",
    "        observed_state_over_time_2d = helper.observe_over_time(real_state_over_time, sigma, \n",
    "                                                               num_obs_per_timestep, nr, nc)\n",
    "\n",
    "        observed_state_over_time = [np.reshape(observed_state_2d, (nr*nc, 1)) \n",
    "                                    for observed_state_2d in observed_state_over_time_2d]\n",
    "\n",
    "        # Run each method\n",
    "        for method_name, method_func, extra_params in methods:\n",
    "            if disp:\n",
    "                print(f\"  Method: {method_name}\")\n",
    "\n",
    "            results = compare_gd_methods_once(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                                x0, num_timesteps, \n",
    "                                                observed_state_over_time, num_iters, step_size, \n",
    "                                                [[method_name, method_func, extra_params]], disp)\n",
    "            \n",
    "            # Include new losses\n",
    "            for method_name, (_, method_losses, method_debug_vars) in results.items():\n",
    "                for loss_name, loss_list in losses[method_name].items():\n",
    "                    loss_list.append(method_losses[loss_name])\n",
    "\n",
    "                for debug_name, debug_list in debug_vars[method_name].items():\n",
    "                    debug_list.append(method_debug_vars[debug_name])\n",
    "\n",
    "    # Average losses\n",
    "\n",
    "    for method_name, method_losses in losses.items():\n",
    "        for loss_name, loss_list in method_losses.items():\n",
    "            losses[method_name][loss_name] = np.mean(loss_list, axis=0)\n",
    "\n",
    "    for method_name, method_debug_vars in debug_vars.items():\n",
    "        for debug_name, debug_list in method_debug_vars.items():\n",
    "            debug_vars[method_name][debug_name] = np.mean(debug_list, axis=0)\n",
    "\n",
    "    return losses, debug_vars\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "\n",
    "    methods = [\n",
    "        [\"Simple GD\", simple_gradient_update_rule, []],\n",
    "        [\"Cholesky GD\", cholesky_update_rule, [C_error]],\n",
    "    ]\n",
    "\n",
    "    losses, debug_vars = compare_gd_methods_many_times(nr, nc, dt, F, gamma, sigma, num_obs_per_timestep, \n",
    "                                    num_timesteps, num_iters, step_size, \n",
    "                                    C_known, C_error, methods, num_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code to allow us to directly compare the losses for each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses_many, num_obs_per_timestep, step_size, num_timesteps, num_iters, min_iter=None, max_iter=None):\n",
    "    \"\"\"\n",
    "    Plots the losses for multiple gradient descent methods over iterations.\n",
    "\n",
    "    Args:\n",
    "    losses_many (dict): Dictionary of losses for each method. \n",
    "                        Keys are method names, values are dictionaries containing losses.\n",
    "    num_obs_per_timestep (int): Number of observations per timestep\n",
    "    step_size (float): Step size used in gradient descent\n",
    "    num_timesteps (int): Number of timesteps in the simulation\n",
    "    num_iters (int): Number of iterations of gradient descent\n",
    "    min_iter (int): Lowest plotted iter (default: None, plots from the beginning)\n",
    "    max_iter (int): Highest plotted iter (default: None, plots until the end)\n",
    "\n",
    "    Returns:\n",
    "    None: This function displays the plot using matplotlib.pyplot.show()\n",
    "\n",
    "    Notes:\n",
    "    Creates a 2x2 grid of plots:\n",
    "    1. Ocean misfit\n",
    "    2. Atmosphere loss\n",
    "    3. Control adjust Mahalanobis distance\n",
    "    4. J' (combined loss for covariance constraint method, ocean loss for others)\n",
    "\n",
    "    Each plot shows the evolution of the respective loss over iterations for all methods.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "    loss_funcs = [\"$\\sum_t (Ex(t)-y(t))^{T} (Ex(t)-y(t))$\", \n",
    "                  \"$\\sum_t (f_i(t)-f_{true}(t) )^{T} ( f_i(t)-f_{true}(t) )$\", \n",
    "                  \"$a_i^T C^{-1} a_i$\"]\n",
    "\n",
    "    # Determine the range of iterations to plot\n",
    "    min_iter = 0 if min_iter is None else max(0, min_iter)\n",
    "    max_iter = num_iters if max_iter is None else min(num_iters, max_iter)\n",
    "    plot_range = slice(min_iter, max_iter)\n",
    "\n",
    "    for i, (loss_name, ax, func) in enumerate(zip([\"ocean_misfit\", \"atmosphere_misfit\", \"mahalanobis(covariance similarity)\"], axs.flatten(), loss_funcs)):\n",
    "        for method_name, losses_dict in losses_many.items():\n",
    "            ax.plot(range(min_iter, max_iter), losses_dict[loss_name][plot_range], label=method_name)\n",
    "        ax.set_xlabel(\"Iteration $i$\")\n",
    "        ax.set_ylabel(loss_name+\" loss:    \"+func)\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{loss_name}: \"+func)\n",
    "        \n",
    "        # Set integer ticks on x-axis\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    # Fourth plot: ocean_misfit + mahalanobis if using covariance control adjust, just ocean otherwise\n",
    "    ax = axs[1, 1]\n",
    "    for method_name, losses_dict in losses_many.items():\n",
    "        if method_name == r\"Covariance Constraint J Gradient Descent\":\n",
    "            combined_loss = [o + c for o, c in zip(losses_dict[\"ocean_misfit\"], losses_dict[\"mahalanobis(covariance similarity)\"])]\n",
    "            ax.plot(range(min_iter, max_iter), combined_loss[plot_range], label=method_name)\n",
    "        else:\n",
    "            ax.plot(range(min_iter, max_iter), losses_dict[\"ocean_misfit\"][plot_range], label=method_name)\n",
    "\n",
    "    ax.set_xlabel(\"Iteration $i$\")\n",
    "    ax.set_ylabel(\"J'\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"J'\")\n",
    "    \n",
    "    # Set integer ticks on x-axis for the fourth plot\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    fig.suptitle(f\"Gradient Descent Variants: num_obs={num_obs_per_timestep}, step_size={step_size}, num_timesteps={num_timesteps}, num_iters={num_iters}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "    plot_losses(losses, num_obs_per_timestep, step_size, num_timesteps, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cholesky seems to probably be a dead end: it doesn't increase covariance, but it performs worse by every other measure.\n",
    "\n",
    "Now that we have the tools we need to directly compare our plots, we can proceed with our other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Include Covariance Constraint in $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use our appended loss function.\n",
    "\n",
    "$$u_i = -\\eta \\cdot \\Bigg( \\frac{dJ}{df} + \\alpha 2C^{-1} a  \\Bigg)$$\n",
    "\n",
    "Our procedure is almost identical: we just add a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_constraint_J_update_rule(curr_iter, s, step_size, f_adjust, C_error, weight_cov_term):\n",
    "    \"\"\"\n",
    "    Computes the update step using a covariance constraint on the loss function.\n",
    "\n",
    "    Args:\n",
    "    curr_iter (int): Current iteration number (unused in this function)\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    f_adjust (np.ndarray): Current adjustment to the forcing field\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    weight_cov_term (float): Weight for the covariance constraint term\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Update step for the forcing field adjustment\n",
    "\n",
    "    Notes:\n",
    "    Adds a weighted covariance constraint term to the original gradient,\n",
    "    then rescales the result to match the original gradient's magnitude.\n",
    "    \"\"\"\n",
    "    cov_term_grad = 2 * np.linalg.inv(C_error) @ f_adjust  # Covariance term\n",
    "\n",
    "    s_prime = s + weight_cov_term * cov_term_grad  # Gradient of J' with respect to the forcing field\n",
    "\n",
    "    #print(np.linalg.norm(s), np.linalg.norm(s_prime))\n",
    "\n",
    "    norm_s_prime = s_prime * (np.linalg.norm(s) / np.linalg.norm(s_prime))  # Rescale magnitude to match original\n",
    "    \n",
    "    return -step_size * norm_s_prime\n",
    "\n",
    "def cov_constraint_J_gradient_descent(M, F, f_true, f_guess, C_known, C_error,       # World parameters\n",
    "                                      x0, timesteps,                                 # Simulation parameters\n",
    "                                      ocean_states_observed, num_iters, step_size,   # Optimization parameters\n",
    "                                      weight_cov_term, disp=False):                  # Optimization method\n",
    "    \"\"\"\n",
    "    Implements gradient descent with a covariance constraint for optimizing the atmospheric forcing field.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    weight_cov_term (float): Weight for the covariance constraint term\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (f_adjust, losses, debug_vars)\n",
    "        f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "        losses (dict): Dictionary of loss values over iterations\n",
    "        debug_vars (dict): Dictionary of debug variables over iterations\n",
    "    \"\"\"\n",
    "    return gradient_descent_template(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                     x0, timesteps, \n",
    "                                     ocean_states_observed, num_iters, step_size,\n",
    "                                     cov_constraint_J_update_rule, [C_error, weight_cov_term], disp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "    weight_cov_term = 0.0001\n",
    "    f_adjust_cov_constraint_J, losses_cov_constraint_J, debug_vars_cov_constraint_J = cov_constraint_J_gradient_descent(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                                                                        x0, num_timesteps, \n",
    "                                                                                        observed_state_over_time, num_iters, step_size, \n",
    "                                                                                        weight_cov_term, disp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "    f_optimized_cov_constraint_J = f_guess + f_adjust_cov_constraint_J\n",
    "\n",
    "    many_states = [f_true, f_guess, f_optimized_cov_constraint_J]\n",
    "    titles = ['True field', 'First-guess field', 'Covariance-constrained field']\n",
    "    big_title = 'Atmospheric Field: Gradient Descent with Covariance Constraint'\n",
    "\n",
    "    helper.plot_multi_heatmap(many_states, nr, nc, titles, big_title, vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_all_code:\n",
    "    methods = [\n",
    "        [\"Simple Gradient Descent\", simple_gradient_update_rule, []],\n",
    "        [\"Cholesky Gradient Descent\", cholesky_update_rule, [C_error]],\n",
    "        [\"Covariance Constraint J Gradient Descent\", cov_constraint_J_update_rule, [C_error, weight_cov_term]],\n",
    "    ]\n",
    "\n",
    "    num_iters_local = 10\n",
    "    step_size_local = step_size / 100\n",
    "    losses, debug_vars = compare_gd_methods_many_times(nr, nc, dt, F, gamma, sigma, num_obs_per_timestep, \n",
    "                                    num_timesteps, num_iters_local, step_size_local, \n",
    "                                    C_known, C_error, methods, num_runs)\n",
    "\n",
    "    plot_losses(losses, num_obs_per_timestep, step_size_local, num_timesteps, num_iters_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dan's Method\n",
    "\n",
    "We've already derived this method above:\n",
    "\n",
    "$$ u_i = \\delta \\Big( \\frac{Cs}{s^\\top C s} \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_update_rule(curr_iter, s, step_size, f_adjust, C_error):\n",
    "    \"\"\"\n",
    "    Computes the update step using Dan's method for improving the Mahalanobis distance.\n",
    "\n",
    "    Args:\n",
    "    curr_iter (int): Current iteration number (unused in this function)\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    f_adjust (np.ndarray): Current adjustment to the forcing field (unused in this function)\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Update step for the forcing field adjustment\n",
    "\n",
    "    Notes:\n",
    "    Modifies the gradient direction to improve the Mahalanobis distance while\n",
    "    maintaining the desired improvement in the loss function J.\n",
    "    \"\"\"\n",
    "    ui_simple_gd = -step_size * s  # Pre-dan step\n",
    "    delta = s.T @ (ui_simple_gd)   # Compute desired improvement of J\n",
    "    \n",
    "    new_vec = (C_error @ s) / (s.T @ C_error @ s)  # Direction modified to improve Mahalanobis distance\n",
    "\n",
    "    return delta * new_vec\n",
    "\n",
    "def dan_gradient_descent(M, F, f_true, f_guess, C_known, C_error,       # World parameters\n",
    "                         x0, timesteps,                                 # Simulation parameters\n",
    "                         ocean_states_observed, num_iters, step_size,   # Optimization parameters\n",
    "                         disp=False):                                   # Optimization method\n",
    "    \"\"\"\n",
    "    Implements gradient descent using Dan's method for optimizing the atmospheric forcing field.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (f_adjust, losses, debug_vars)\n",
    "        f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "        losses (dict): Dictionary of loss values over iterations\n",
    "        debug_vars (dict): Dictionary of debug variables over iterations\n",
    "    \"\"\"\n",
    "    return gradient_descent_template(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                     x0, timesteps, \n",
    "                                     ocean_states_observed, num_iters, step_size,\n",
    "                                     dan_update_rule, [C_error], disp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_field_view = False\n",
    "\n",
    "if run_all_code or dan_field_view:\n",
    "\n",
    "    f_adjust_dan, losses_dan, debug_vars_dan = dan_gradient_descent(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                                                    x0, num_timesteps, \n",
    "                                                                    observed_state_over_time, num_iters, step_size, \n",
    "                                                                    disp=True)\n",
    "\n",
    "    # Plot the true field, the first guess, and the improved field\n",
    "\n",
    "    f_optimized_dan = f_guess + f_adjust_dan\n",
    "\n",
    "    many_states = [f_true, f_guess, f_optimized_dan]\n",
    "    titles = ['True field', 'First-guess field', 'Dan field']\n",
    "    big_title = 'Atmospheric Field: Gradient Descent with Dan'\n",
    "\n",
    "    helper.plot_multi_heatmap(many_states, nr, nc, titles, big_title, vmin=None, vmax=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "dan_methods_view = False\n",
    "\n",
    "if run_all_code or dan_methods_view:\n",
    "        \n",
    "        weight_cov_term = 0.0001\n",
    "    \n",
    "        methods = [\n",
    "            #[\"Simple Gradient Descent\", simple_gradient_update_rule, []],\n",
    "            #[\"Cholesky Gradient Descent\", cholesky_update_rule, [C_error]],\n",
    "            #[\"Covariance Constraint J Gradient Descent\", cov_constraint_J_update_rule, [C_error, weight_cov_term]],\n",
    "            [\"Dan Gradient Descent\", dan_update_rule, [C_error]],\n",
    "        ]\n",
    "    \n",
    "        num_iters_local = 250\n",
    "        step_size_local = step_size\n",
    "        losses, debug_vars = compare_gd_methods_many_times(nr, nc, dt, F, gamma, sigma, num_obs_per_timestep, \n",
    "                                        num_timesteps, num_iters_local, step_size_local, \n",
    "                                        C_known, C_error, methods, num_runs)\n",
    "    \n",
    "        plot_losses(losses, num_obs_per_timestep, step_size_local, num_timesteps, num_iters_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, Dan's Method appears to be, by far, the most successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dan's Method Modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our modified update rule is given by:\n",
    "\n",
    "$$u_i = -a_i + \\Big(\\delta + a_i^\\top s\\Big) \\Big( \\frac{ Cs }{s^\\top C s} \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dan_modified_update_rule(curr_iter, s, step_size, f_adjust, C_error):\n",
    "    \"\"\"\n",
    "    Computes the update step using a modified version of Dan's method for improving the Mahalanobis distance.\n",
    "\n",
    "    Args:\n",
    "    curr_iter (int): Current iteration number (unused in this function)\n",
    "    s (np.ndarray): Gradient of the loss with respect to the forcing field\n",
    "    step_size (float): Step size for gradient descent\n",
    "    f_adjust (np.ndarray): Current adjustment to the forcing field\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Update step for the forcing field adjustment\n",
    "\n",
    "    Notes:\n",
    "    Modifies the gradient direction to improve the Mahalanobis distance while\n",
    "    maintaining the desired improvement in the loss function J. This version\n",
    "    intends to improve Mahalanobis distance of a_i+u_i (f_adjust+gradient update), instead of just u_i.\n",
    "    \"\"\"\n",
    "    ui_simple_gd = -step_size * s  # Pre-dan step\n",
    "    delta = s.T @ (ui_simple_gd)   # Compute desired improvement of J\n",
    "    \n",
    "    new_vec = (C_error @ s) / (s.T @ C_error @ s)  # Direction modified to improve Mahalanobis distance\n",
    "    vec_scale = delta + s.T @ f_adjust\n",
    "\n",
    "    return -f_adjust + vec_scale * new_vec\n",
    "\n",
    "def dan_modified_gradient_descent(M, F, f_true, f_guess, C_known, C_error,       # World parameters\n",
    "                                  x0, timesteps,                                 # Simulation parameters\n",
    "                                  ocean_states_observed, num_iters, step_size,   # Optimization parameters\n",
    "                                  disp=False):                                   # Optimization method\n",
    "    \"\"\"\n",
    "    Implements gradient descent using a modified version of Dan's method for optimizing the atmospheric forcing field.\n",
    "\n",
    "    Args:\n",
    "    M (np.ndarray): Model matrix\n",
    "    F (float): Forcing coefficient\n",
    "    f_true (np.ndarray): True atmospheric forcing field\n",
    "    f_guess (np.ndarray): Initial guess for the atmospheric forcing field\n",
    "    C_known (np.ndarray): Covariance matrix for the known portion of the control\n",
    "    C_error (np.ndarray): Covariance matrix for the control error\n",
    "    x0 (np.ndarray): Initial ocean state\n",
    "    timesteps (int): Number of timesteps for simulation\n",
    "    ocean_states_observed (list): List of observed ocean states\n",
    "    num_iters (int): Number of iterations for gradient descent\n",
    "    step_size (float): Step size for gradient descent\n",
    "    disp (bool, optional): If True, display progress. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (f_adjust, losses, debug_vars)\n",
    "        f_adjust (np.ndarray): Final adjustment to the atmospheric forcing field\n",
    "        losses (dict): Dictionary of loss values over iterations\n",
    "        debug_vars (dict): Dictionary of debug variables over iterations\n",
    "    \"\"\"\n",
    "    return gradient_descent_template(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                     x0, timesteps, \n",
    "                                     ocean_states_observed, num_iters, step_size,\n",
    "                                     dan_modified_update_rule, [C_error], disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_modified_field_view = False\n",
    "\n",
    "if run_all_code or dan_modified_field_view:\n",
    "    \n",
    "        f_adjust_dan_modified, losses_dan_modified, debug_vars_dan_modified = dan_modified_gradient_descent(M, F, f_true, f_guess, C_known, C_error,\n",
    "                                                                        x0, num_timesteps, \n",
    "                                                                        observed_state_over_time, num_iters, step_size, \n",
    "                                                                        disp=True)\n",
    "    \n",
    "        # Plot the true field, the first guess, and the improved field\n",
    "    \n",
    "        f_optimized_dan_modified = f_guess + f_adjust_dan_modified\n",
    "    \n",
    "        many_states = [f_true, f_guess, f_optimized_dan_modified]\n",
    "        titles = ['True field', 'First-guess field', 'Dan modified field']\n",
    "        big_title = 'Atmospheric Field: Gradient Descent with Dan Modified'\n",
    "    \n",
    "        helper.plot_multi_heatmap(many_states, nr, nc, titles, big_title, vmin=None, vmax=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_modified_methods_view = True\n",
    "\n",
    "if run_all_code or dan_modified_methods_view:\n",
    "\n",
    "    weight_cov_term = 1e-6\n",
    "\n",
    "    methods = [\n",
    "        #[\"Simple Gradient Descent\", simple_gradient_update_rule, []],\n",
    "        #[\"Cholesky Gradient Descent\", cholesky_update_rule, [C_error]],\n",
    "        #[\"Modified J Gradient Descent $\\\\alpha = 1e-6$\", cov_constraint_J_update_rule, [C_error, 1e-6]],\n",
    "        #[\"Modified J Gradient Descent $\\alpha = 5e-6$\", cov_constraint_J_update_rule, [C_error, 5e-6]],\n",
    "        #[\"Modified J Gradient Descent $\\alpha = 5e-7$\", cov_constraint_J_update_rule, [C_error, 5e-7]],\n",
    "        \n",
    "        #[\"Dan Gradient Descent\", dan_update_rule, [C_error]],\n",
    "        [\"Dan Modified Gradient Descent\", dan_modified_update_rule, [C_error]],\n",
    "    ]\n",
    "\n",
    "    num_iters_local = 250\n",
    "    step_size_local = step_size*10\n",
    "    losses, debug_vars = compare_gd_methods_many_times(nr, nc, dt, F, gamma, sigma, num_obs_per_timestep, \n",
    "                                    num_timesteps, num_iters_local, step_size_local, \n",
    "                                    C_known, C_error, methods, num_runs, disp=True)\n",
    "\n",
    "    plot_losses(losses, num_obs_per_timestep, step_size_local, num_timesteps, num_iters_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is... really unstable, for some reason. We see it aggressively oscillating. But why? We'll explore that in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
