{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Goal\n",
    "\n",
    "Our model is designed to compute:\n",
    "\n",
    "$x_{t+1}$ from $x_t$\n",
    "\n",
    "So we can directly predict the next state of our system given the current state.\n",
    "\n",
    "Unlike the base model, we will use a single set of fixed parameters for the entire dataset, rather than generating new parameters for each datapoint.\n",
    "\n",
    "# Our Data\n",
    "\n",
    "Our model receives:\n",
    "- $x_t$: Current state\n",
    "\n",
    "And attempts to predict:\n",
    "- $x_{t+1}$: The next state\n",
    "\n",
    "We generate our data by:\n",
    "- First establishing fixed physical parameters for the entire dataset\n",
    "- Then for each datapoint:\n",
    "  - Generating a new, random initial state $x_t$\n",
    "  - Running it forward one timestep to get $x_{t+1}$\n",
    "  - Using this pair to create one datapoint\n",
    "  - Repeat with new random initial states but same fixed parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "import helper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, nr: int, nc: int, dt: float, F: float):\n",
    "        \"\"\"\n",
    "        Initialize dataset with fixed parameters for all samples.\n",
    "        \n",
    "        Args:\n",
    "            num_samples (int): Number of samples to generate\n",
    "            nr (int): Number of rows in grid\n",
    "            nc (int): Number of columns in grid\n",
    "            dt (float): Time step size\n",
    "            F (float): Forcing parameter\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        self.dt = dt\n",
    "        self.F = F\n",
    "        \n",
    "        # Generate fixed parameters once (used internally but not stored in samples)\n",
    "        self.fixed_params = self._generate_params()\n",
    "        self.samples = self._generate_samples()\n",
    "\n",
    "    def _generate_gaussian_field(self, n, nrv, ncv):\n",
    "        \"\"\"\n",
    "        Generate a Gaussian field composed of n Gaussian functions.\n",
    "        \"\"\"\n",
    "        mux = np.random.choice(ncv, n)\n",
    "        muy = np.random.choice(range(2, nrv - 2), n)\n",
    "        sigmax = np.random.uniform(1, ncv/4, n)\n",
    "        sigmay = np.random.uniform(1, nrv/4, n)\n",
    "\n",
    "        v = np.zeros((nrv, ncv))\n",
    "        for i in range(n):\n",
    "            for x in range(ncv):\n",
    "                for y in range(nrv):\n",
    "                    # Create three copies for pseudo-periodic field\n",
    "                    gauss = np.exp(-((x-mux[i])**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]-ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    gauss += np.exp(-((x-(mux[i]+ncv))**2/(2*sigmax[i]**2) + (y-muy[i])**2/(2*sigmay[i]**2)))\n",
    "                    v[y,x] += gauss\n",
    "        return v\n",
    "\n",
    "    def _generate_circular_field(self, v):\n",
    "        \"\"\"\n",
    "        Generate a circular field from gradient of input field.\n",
    "        \"\"\"\n",
    "        grad_v_y, grad_v_x = np.gradient(v)\n",
    "        return -grad_v_y, grad_v_x\n",
    "\n",
    "    def _generate_params(self):\n",
    "        \"\"\"\n",
    "        Generate fixed model parameters to be used for all samples.\n",
    "        \"\"\"\n",
    "        # Base grid parameters\n",
    "        DX_C = torch.ones(self.nr, self.nc + 1)\n",
    "        DY_C = torch.ones(self.nr + 1, self.nc)\n",
    "        DX_G = torch.ones(self.nr + 1, self.nc)\n",
    "        DY_G = torch.ones(self.nr, self.nc + 1)\n",
    "        RAC = torch.ones(self.nr, self.nc)\n",
    "\n",
    "        # Generate random diffusivities (must be positive)\n",
    "        KX = torch.abs(torch.rand(self.nr, self.nc + 1))\n",
    "        KY = torch.abs(torch.rand(self.nr + 1, self.nc))\n",
    "\n",
    "        # Generate velocities using Gaussian field approach\n",
    "        num_gauss = 16  # Number of Gaussian functions for velocity field\n",
    "        gauss = self._generate_gaussian_field(num_gauss, self.nr + 1, self.nc + 1)\n",
    "        VX_np, VY_np = self._generate_circular_field(gauss)\n",
    "        \n",
    "        # Convert velocities to PyTorch and scale\n",
    "        VX = torch.from_numpy(100 * VX_np[:-1, :]).float()\n",
    "        VY = torch.from_numpy(100 * VY_np[:, :-1]).float()\n",
    "\n",
    "        # Create random forcing term\n",
    "        f = torch.randn(self.nr * self.nc)\n",
    "\n",
    "        return {\n",
    "            'KX': KX,\n",
    "            'KY': KY,\n",
    "            'DX_C': DX_C,\n",
    "            'DY_C': DY_C,\n",
    "            'DX_G': DX_G,\n",
    "            'DY_G': DY_G,\n",
    "            'VX': VX,\n",
    "            'VY': VY,\n",
    "            'RAC': RAC,\n",
    "            'f': f,\n",
    "        }\n",
    "\n",
    "    def _compute_next_state(self, x_t):\n",
    "        \"\"\"\n",
    "        Compute x(t+1) using fixed parameters.\n",
    "        \"\"\"\n",
    "        # Convert PyTorch tensors to numpy arrays for helper function\n",
    "        np_params = {\n",
    "            key: tensor.cpu().detach().numpy() if torch.is_tensor(tensor) else tensor\n",
    "            for key, tensor in self.fixed_params.items()\n",
    "        }\n",
    "        \n",
    "        # Get model matrix M using helper function\n",
    "        M = helper.make_M_2d_diffusion_advection_forcing(\n",
    "            nr=self.nr,\n",
    "            nc=self.nc,\n",
    "            dt=self.dt,\n",
    "            KX=np_params['KX'],\n",
    "            KY=np_params['KY'],\n",
    "            DX_C=np_params['DX_C'],\n",
    "            DY_C=np_params['DY_C'], \n",
    "            DX_G=np_params['DX_G'],\n",
    "            DY_G=np_params['DY_G'],\n",
    "            VX=np_params['VX'],\n",
    "            VY=np_params['VY'],\n",
    "            RAC=np_params['RAC'],\n",
    "            F=self.F,\n",
    "            cyclic_east_west=True,\n",
    "            cyclic_north_south=False,\n",
    "            M_is_sparse=False\n",
    "        )\n",
    "        \n",
    "        # Convert to numpy arrays and do matrix multiplication\n",
    "        x_t_np = x_t.cpu().detach().numpy()\n",
    "        f_np = np_params['f']\n",
    "        \n",
    "        # Compute next state: x(t+1) = Mx(t) + Ff\n",
    "        result_np = M @ x_t_np + self.F * f_np\n",
    "        \n",
    "        # Convert back to PyTorch tensor\n",
    "        return torch.from_numpy(result_np).float()\n",
    "\n",
    "    def _generate_samples(self) -> list:\n",
    "        \"\"\"Generate multiple samples using fixed parameters but only store states.\"\"\"\n",
    "        samples = []\n",
    "        for _ in range(self.num_samples):\n",
    "            if _ % 10000 == 0:\n",
    "                print(f\"Generating sample {_+1}/{self.num_samples}\")\n",
    "            x_t = torch.randn(self.nr * self.nc)  # Random initial state\n",
    "            x_t_plus_1 = self._compute_next_state(x_t)  # Next state\n",
    "            samples.append((x_t, x_t_plus_1))  # Only store the states\n",
    "        return samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.samples[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateModelNet(torch.nn.Module):\n",
    "    def __init__(self, nr: int, nc: int):\n",
    "        super().__init__()\n",
    "        self.nr = nr\n",
    "        self.nc = nc\n",
    "        \n",
    "        self.state_size = nr * nc\n",
    "        self.input_size = self.state_size  # Now only takes state as input\n",
    "        self.output_size = self.state_size\n",
    "        \n",
    "        print(f\"Input size: {self.input_size}\")\n",
    "        \n",
    "        # Original simple architecture with two layers\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, self.output_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Ensure x has correct shape\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_dataset(\n",
    "    filename: str,\n",
    "    num_train_samples: int,\n",
    "    num_test_samples: int,\n",
    "    nr: int,\n",
    "    nc: int,\n",
    "    dt: float,\n",
    "    F: float\n",
    ") -> None:\n",
    "    # Create dataset instances\n",
    "    train_dataset = StateModelDataset(num_train_samples, nr, nc, dt, F)\n",
    "    test_dataset = StateModelDataset(num_test_samples, nr, nc, dt, F)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        # Create train and test groups\n",
    "        train_group = f.create_group('train')\n",
    "        test_group = f.create_group('test')\n",
    "        \n",
    "        # Save metadata\n",
    "        f.attrs['nr'] = nr\n",
    "        f.attrs['nc'] = nc\n",
    "        f.attrs['dt'] = dt\n",
    "        f.attrs['F'] = F\n",
    "        \n",
    "        # Helper function to save a single dataset\n",
    "        def save_dataset(group, dataset, desc):\n",
    "            # Save samples directly without params\n",
    "            samples_group = group.create_group('samples')\n",
    "            for i, (x_t, x_t_plus_1) in enumerate(tqdm(dataset, desc=desc)):\n",
    "                sample_group = samples_group.create_group(f'sample_{i}')\n",
    "                sample_group.create_dataset('x_t', data=x_t.numpy())\n",
    "                sample_group.create_dataset('x_t_plus_1', data=x_t_plus_1.numpy())\n",
    "        \n",
    "        # Save training and testing datasets\n",
    "        save_dataset(train_group, train_dataset, \"Saving training data\")\n",
    "        save_dataset(test_group, test_dataset, \"Saving testing data\")\n",
    "\n",
    "def load_dataset(filename: str) -> Tuple[Dict, Dict]:\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        metadata = {\n",
    "            'nr': f.attrs['nr'],\n",
    "            'nc': f.attrs['nc'],\n",
    "            'dt': f.attrs['dt'],\n",
    "            'F': f.attrs['F']\n",
    "        }\n",
    "        \n",
    "        def load_dataset(group):\n",
    "            data = []\n",
    "            samples_group = group['samples']\n",
    "            for sample_name in tqdm(samples_group.keys(), desc=f\"Loading {group.name} data\"):\n",
    "                sample = samples_group[sample_name]\n",
    "                \n",
    "                # Load only states\n",
    "                x_t = torch.from_numpy(sample['x_t'][:]).float()\n",
    "                x_t_plus_1 = torch.from_numpy(sample['x_t_plus_1'][:]).float()\n",
    "                \n",
    "                # Add to data list without fixed parameters\n",
    "                data.append((x_t, x_t_plus_1))\n",
    "                \n",
    "            return data\n",
    "        \n",
    "        train_data = load_dataset(f['train'])\n",
    "        test_data = load_dataset(f['test'])\n",
    "    \n",
    "    return {\n",
    "        'train': train_data,\n",
    "        'metadata': metadata\n",
    "    }, {\n",
    "        'test': test_data,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "class SavedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for loading pre-saved data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def create_dataloaders(train_data, test_data, batch_size=32, shuffle_train=True):\n",
    "    \"\"\"\n",
    "    Create DataLoader objects for training and testing datasets.\n",
    "    Now handles data without parameters.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training dataset\n",
    "        test_data: Testing dataset\n",
    "        batch_size: Batch size for DataLoaders\n",
    "        shuffle_train: Whether to shuffle training data\n",
    "        \n",
    "    Returns:\n",
    "        train_loader, test_loader: DataLoader objects\n",
    "    \"\"\"\n",
    "    train_dataset = SavedDataset(train_data['train'])\n",
    "    test_dataset = SavedDataset(test_data['test'])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Adjust based on system\n",
    "        pin_memory=True  # Helps with GPU transfer\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save datasets\n",
    "\"\"\"\n",
    "generate_and_save_dataset(\n",
    "    filename='state_model_data_one-world.h5',\n",
    "    num_train_samples=180000,\n",
    "    num_test_samples=20000,\n",
    "    nr=10,\n",
    "    nc=10,\n",
    "    dt=0.1,\n",
    "    F=1.0\n",
    ")\n",
    "\n",
    "# Load dataset and create dataloaders\n",
    "train_data, test_data = load_dataset('state_model_data_one-world.h5')\n",
    "train_loader, test_loader = create_dataloaders(\n",
    "    train_data, \n",
    "    test_data, \n",
    "    batch_size=32\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: str = 'cpu',\n",
    "    run_number: int = None\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    run_str = f\" (Run {run_number})\" if run_number is not None else \"\"\n",
    "    print(f\"Training started{run_str}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x_t, x_t_plus_1 in train_dataloader:  # Now unpacking only states\n",
    "            x_t = x_t.to(device)\n",
    "            x_t_plus_1 = x_t_plus_1.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predicted = model(x_t)\n",
    "            loss = criterion(predicted, x_t_plus_1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_t, x_t_plus_1 in test_dataloader:\n",
    "                x_t = x_t.to(device)\n",
    "                x_t_plus_1 = x_t_plus_1.to(device)\n",
    "                \n",
    "                predicted = model(x_t)\n",
    "                test_loss = criterion(predicted, x_t_plus_1)\n",
    "                total_test_loss += test_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {avg_train_loss:.6f}\")\n",
    "            print(f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    print(f\"Training complete{run_str}!\")\n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "def train_multiple_models(\n",
    "    nr: int, \n",
    "    nc: int, \n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    num_runs: int = 10,\n",
    "    num_epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    all_models = []\n",
    "    all_train_losses = []\n",
    "    all_test_losses = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"\\nStarting model {i+1}/{num_runs}\")\n",
    "        model = StateModelNet(nr, nc)\n",
    "        \n",
    "        trained_model, train_losses, test_losses = train_and_evaluate_model(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            device=device,\n",
    "            run_number=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot individual run results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs = range(1, num_epochs + 1)\n",
    "        plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, test_losses, 'r-', label='Test Loss')\n",
    "        plt.title(f'Training and Testing Loss Over Time - Run {i+1}')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        all_models.append(trained_model)\n",
    "        all_train_losses.append(train_losses)\n",
    "        all_test_losses.append(test_losses)\n",
    "    \n",
    "    return all_models, all_train_losses, all_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset('state_model_data_one-world.h5')\n",
    "train_loader = DataLoader(SavedDataset(train_data['train']), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(SavedDataset(test_data['test']), batch_size=32, shuffle=False)\n",
    "\n",
    "# Train multiple models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "models, train_losses, test_losses = train_multiple_models(\n",
    "    nr=10, \n",
    "    nc=10, \n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    num_runs=10,\n",
    "    num_epochs=10,\n",
    "    device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
